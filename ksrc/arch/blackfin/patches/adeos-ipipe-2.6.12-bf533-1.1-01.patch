diff -uNrp 2.6.12/arch/bfinnommu/Kconfig 2.6.12-ipipe/arch/bfinnommu/Kconfig
--- 2.6.12/arch/bfinnommu/Kconfig	2005-09-16 06:20:17.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/Kconfig	2005-12-27 19:33:14.000000000 +0100
@@ -525,6 +525,8 @@ source "drivers/pci/hotplug/Kconfig"
 
 endmenu
 
+source "kernel/ipipe/Kconfig"
+
 menu	"Executable File Formats"
 source	"fs/Kconfig.binfmt"
 endmenu
diff -uNrp 2.6.12/arch/bfinnommu/Makefile 2.6.12-ipipe/arch/bfinnommu/Makefile
--- 2.6.12/arch/bfinnommu/Makefile	2005-06-13 10:44:27.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/Makefile	2006-01-07 09:56:44.000000000 +0100
@@ -8,7 +8,7 @@
 # Copyright (C) 2004 LG Soft India 
 
 CROSS_COMPILE	:= bfin-uclinux-
-LDFLAGS_vmlinux	:= -M -X
+LDFLAGS_vmlinux	:= -X
 LDFLAGS_BLOB	:=--format binary -elf32-bfin
 OBJCOPYFLAGS	:=-O binary -R .note -R .comment -S
 GZFLAGS		:=-9
diff -uNrp 2.6.12/arch/bfinnommu/kernel/Makefile 2.6.12-ipipe/arch/bfinnommu/kernel/Makefile
--- 2.6.12/arch/bfinnommu/kernel/Makefile	2005-08-12 04:42:20.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/Makefile	2005-12-27 19:33:14.000000000 +0100
@@ -8,4 +8,5 @@ obj-y:= entry.o process.o bfin_ksyms.o p
 obj-$(CONFIG_MODULES)		+= module.o
 obj-$(CONFIG_BLKFIN_DMA)	+= dma.o
 obj-$(CONFIG_BLKFIN_SIMPLE_DMA)	+= simple_dma.o
+obj-$(CONFIG_IPIPE)		+= ipipe-core.o ipipe-root.o
 
diff -uNrp 2.6.12/arch/bfinnommu/kernel/ipipe-core.c 2.6.12-ipipe/arch/bfinnommu/kernel/ipipe-core.c
--- 2.6.12/arch/bfinnommu/kernel/ipipe-core.c	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/ipipe-core.c	2006-01-04 17:39:14.000000000 +0100
@@ -0,0 +1,324 @@
+/* -*- linux-c -*-
+ * linux/arch/bfinnommu/kernel/ipipe-core.c
+ *
+ * Copyright (C) 2005 Philippe Gerum.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ * USA; either version 2 of the License, or (at your option) any later
+ * version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Architecture-dependent I-PIPE core support for the Blackfin/NOMMU.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/bitops.h>
+#include <linux/module.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/irqchip.h>
+#include <asm/io.h>
+
+extern struct irqdesc irq_desc[];
+
+struct pt_regs __ipipe_tick_regs[IPIPE_NR_CPUS];
+
+#ifdef CONFIG_SMP
+
+static cpumask_t __ipipe_cpu_sync_map;
+
+static cpumask_t __ipipe_cpu_lock_map;
+
+static ipipe_spinlock_t __ipipe_cpu_barrier = IPIPE_SPIN_LOCK_UNLOCKED;
+
+static atomic_t __ipipe_critical_count = ATOMIC_INIT(0);
+
+static void (*__ipipe_cpu_sync) (void);
+
+/* Always called with hw interrupts off. */
+
+void __ipipe_do_critical_sync(unsigned irq)
+{
+	ipipe_declare_cpuid;
+
+	ipipe_load_cpuid();
+
+	cpu_set(cpuid, __ipipe_cpu_sync_map);
+
+	/* Now we are in sync with the lock requestor running on another
+	   CPU. Enter a spinning wait until he releases the global
+	   lock. */
+	spin_lock_hw(&__ipipe_cpu_barrier);
+
+	/* Got it. Now get out. */
+
+	if (__ipipe_cpu_sync)
+		/* Call the sync routine if any. */
+		__ipipe_cpu_sync();
+
+	spin_unlock_hw(&__ipipe_cpu_barrier);
+
+	cpu_clear(cpuid, __ipipe_cpu_sync_map);
+}
+
+#endif	/* CONFIG_SMP */
+
+unsigned long ipipe_critical_enter(void (*syncfn) (void))
+{
+	unsigned long flags;
+
+	local_irq_save_hw(flags);
+
+#ifdef CONFIG_SMP
+	if (num_online_cpus() > 1) {	/* We might be running a SMP-kernel on a UP box... */
+		ipipe_declare_cpuid;
+		cpumask_t lock_map;
+
+		ipipe_load_cpuid();
+
+		if (!cpu_test_and_set(cpuid, __ipipe_cpu_lock_map)) {
+			while (cpu_test_and_set
+			       (BITS_PER_LONG - 1, __ipipe_cpu_lock_map)) {
+				int n = 0;
+				do {
+					cpu_relax();
+				} while (++n < cpuid);
+			}
+
+			spin_lock_hw(&__ipipe_cpu_barrier);
+
+			__ipipe_cpu_sync = syncfn;
+
+			/* Send the sync IPI to all processors but the current one. */
+			send_IPI_allbutself(IPIPE_CRITICAL_VECTOR);
+
+			cpus_andnot(lock_map, cpu_online_map,
+				    __ipipe_cpu_lock_map);
+
+			while (!cpus_equal(__ipipe_cpu_sync_map, lock_map))
+				cpu_relax();
+		}
+
+		atomic_inc(&__ipipe_critical_count);
+	}
+#endif	/* CONFIG_SMP */
+
+	return flags;
+}
+
+void ipipe_critical_exit(unsigned long flags)
+{
+#ifdef CONFIG_SMP
+	if (num_online_cpus() > 1) {	/* We might be running a SMP-kernel on a UP box... */
+		ipipe_declare_cpuid;
+
+		ipipe_load_cpuid();
+
+		if (atomic_dec_and_test(&__ipipe_critical_count)) {
+			spin_unlock_hw(&__ipipe_cpu_barrier);
+
+			while (!cpus_empty(__ipipe_cpu_sync_map))
+				cpu_relax();
+
+			cpu_clear(cpuid, __ipipe_cpu_lock_map);
+			cpu_clear(BITS_PER_LONG - 1, __ipipe_cpu_lock_map);
+		}
+	}
+#endif	/* CONFIG_SMP */
+
+	local_irq_restore_hw(flags);
+}
+
+/*
+ * __ipipe_sync_stage() -- Flush the pending IRQs for the current
+ * domain (and processor). This routine flushes the interrupt log
+ * (see "Optimistic interrupt protection" from D. Stodolsky et al. for
+ * more on the deferred interrupt scheme). Every interrupt that
+ * occurred while the pipeline was stalled gets played. WARNING:
+ * callers on SMP boxen should always check for CPU migration on
+ * return of this routine. One can control the kind of interrupts
+ * which are going to be sync'ed using the syncmask
+ * parameter. IPIPE_IRQMASK_ANY plays them all, IPIPE_IRQMASK_VIRT
+ * plays virtual interrupts only. This routine must be called with hw
+ * interrupts off.
+ */
+void __ipipe_sync_stage(unsigned long syncmask)
+{
+	unsigned long mask, submask;
+	struct ipcpudata *cpudata;
+	struct ipipe_domain *ipd;
+	ipipe_declare_cpuid;
+	int level, rank;
+	unsigned irq;
+
+	ipipe_load_cpuid();
+	ipd = ipipe_percpu_domain[cpuid];
+	cpudata = &ipd->cpudata[cpuid];
+
+	if (__test_and_set_bit(IPIPE_SYNC_FLAG, &cpudata->status))
+		return;
+
+	/*
+	 * The policy here is to keep the dispatching code interrupt-free
+	 * by stalling the current stage. If the upper domain handler
+	 * (which we call) wants to re-enable interrupts while in a safe
+	 * portion of the code (e.g. SA_INTERRUPT flag unset for Linux's
+	 * sigaction()), it will have to unstall (then stall again before
+	 * returning to us!) the stage when it sees fit.
+	 */
+	while ((mask = (cpudata->irq_pending_hi & syncmask)) != 0) {
+		level = ffs(mask) - 1;
+		__clear_bit(level, &cpudata->irq_pending_hi);
+
+		while ((submask = cpudata->irq_pending_lo[level]) != 0) {
+
+			if (ipd == ipipe_root_domain &&
+			    test_bit(IPIPE_ROOTLOCK_FLAG, &ipd->flags)) {
+				__set_bit(level, &cpudata->irq_pending_hi);
+				goto done;
+			}
+
+			rank = ffs(submask) - 1;
+			irq = (level << IPIPE_IRQ_ISHIFT) + rank;
+
+			if (test_bit(IPIPE_LOCK_FLAG, &ipd->irqs[irq].control)) {
+				__clear_bit(rank,
+					    &cpudata->irq_pending_lo[level]);
+				continue;
+			}
+
+			if (--cpudata->irq_counters[irq].pending_hits == 0) {
+				__clear_bit(rank,
+					    &cpudata->irq_pending_lo[level]);
+				ipipe_mark_irq_delivery(ipd,irq,cpuid);
+			}
+
+			__set_bit(IPIPE_STALL_FLAG, &cpudata->status);
+			ipipe_mark_domain_stall(ipd, cpuid);
+
+			if (ipd == ipipe_root_domain) {
+				/*
+				 * Note: the I-pipe implements a
+				 * threaded interrupt model on this
+				 * arch for Linux external IRQs. The
+				 * interrupt handler we call here only
+				 * wakes up the associated IRQ thread.
+				 */
+				if (ipipe_virtual_irq_p(irq)) {
+					local_irq_enable_hw();
+					ipd->irqs[irq].handler(irq, ipd->irqs[irq].cookie);
+					local_irq_disable_hw();
+				} else
+					((void (*)(unsigned, struct pt_regs *))
+					 ipd->irqs[irq].handler) (irq, __ipipe_tick_regs + cpuid);
+			} else {
+				__clear_bit(IPIPE_SYNC_FLAG, &cpudata->status);
+				ipd->irqs[irq].handler(irq, ipd->irqs[irq].cookie);
+				__set_bit(IPIPE_SYNC_FLAG, &cpudata->status);
+			}
+#ifdef CONFIG_SMP
+			{
+				int _cpuid = ipipe_processor_id();
+
+				if (_cpuid != cpuid) {	/* Handle CPU migration. */
+					/* We expect any domain to clear the SYNC bit each
+					   time it switches in a new task, so that preemptions
+					   and/or CPU migrations (in the SMP case) over the
+					   ISR do not lock out the log syncer for some
+					   indefinite amount of time. In the Linux case,
+					   schedule() handles this (see kernel/sched.c). For
+					   this reason, we don't bother clearing it here for
+					   the source CPU in the migration handling case,
+					   since it must have scheduled another task in by
+					   now. */
+					cpuid = _cpuid;
+					cpudata = &ipd->cpudata[cpuid];
+					__set_bit(IPIPE_SYNC_FLAG,&cpudata->status);
+				}
+			}
+#endif	/* CONFIG_SMP */
+			__clear_bit(IPIPE_STALL_FLAG, &cpudata->status);
+			ipipe_mark_domain_unstall(ipd, cpuid);
+		}
+	}
+
+done:
+	__clear_bit(IPIPE_SYNC_FLAG, &cpudata->status);
+}
+
+int ipipe_get_sysinfo(struct ipipe_sysinfo *info)
+{
+	info->ncpus = num_online_cpus();
+	info->cpufreq = ipipe_cpu_freq();
+	info->archdep.tmirq = IPIPE_TIMER_IRQ;
+	info->archdep.tmfreq = info->cpufreq;
+
+	return 0;
+}
+
+/*
+ * ipipe_trigger_irq() -- Push the interrupt at front of the pipeline
+ * just like if it has been actually received from a hw source. Also
+ * works for virtual interrupts.
+ */
+int ipipe_trigger_irq(unsigned irq)
+{
+	unsigned long flags;
+
+	if (irq >= IPIPE_NR_IRQS ||
+	    (ipipe_virtual_irq_p(irq)
+	     && !test_bit(irq - IPIPE_VIRQ_BASE, &__ipipe_virtual_irq_map)))
+		return -EINVAL;
+
+	local_irq_save_hw(flags);
+
+	__ipipe_handle_irq(irq, NULL);
+
+	local_irq_restore_hw(flags);
+
+	return 1;
+}
+
+int ipipe_tune_timer(unsigned long ns, int flags)
+{
+	unsigned long x, hz;
+
+	x = ipipe_critical_enter(NULL);
+
+	*pTIMER_DISABLE = 1;
+	__builtin_bfin_ssync();
+	*pTIMER0_CONFIG = 0x19;	/* IRQ enable, periodic, PWM_OUT, SCLKed */
+	__builtin_bfin_ssync();
+	hz = (flags & IPIPE_RESET_TIMER) ? HZ : 1000000000L / ns;
+	*pTIMER0_PERIOD = get_sclk() / hz;
+	__builtin_bfin_ssync();
+	*pTIMER0_WIDTH = 1;
+	__builtin_bfin_ssync();
+	*pTIMER_ENABLE = 1;
+	__builtin_bfin_ssync();
+
+	ipipe_critical_exit(x);
+
+	return 0;
+}
+
+EXPORT_SYMBOL(__ipipe_sync_stage);
+EXPORT_SYMBOL(ipipe_critical_enter);
+EXPORT_SYMBOL(ipipe_critical_exit);
+EXPORT_SYMBOL(ipipe_trigger_irq);
+EXPORT_SYMBOL(ipipe_get_sysinfo);
+EXPORT_SYMBOL(ipipe_tune_timer);
diff -uNrp 2.6.12/arch/bfinnommu/kernel/ipipe-root.c 2.6.12-ipipe/arch/bfinnommu/kernel/ipipe-root.c
--- 2.6.12/arch/bfinnommu/kernel/ipipe-root.c	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/ipipe-root.c	2006-01-06 12:46:49.000000000 +0100
@@ -0,0 +1,335 @@
+/* -*- linux-c -*-
+ * linux/arch/bfinnommu/kernel/ipipe-root.c
+ *
+ * Copyright (C) 2005 Philippe Gerum.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ * USA; either version 2 of the License, or (at your option) any later
+ * version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Architecture-dependent I-pipe support for the Blackfin/NOMMU.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/errno.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/io.h>
+
+irqreturn_t timer_interrupt(int irq, struct pt_regs * regs);
+
+asmlinkage void asm_do_IRQ(unsigned int irq, struct pt_regs *regs);
+
+extern struct irqdesc irq_desc[];
+
+unsigned long __ipipe_core_clock;
+
+unsigned long __ipipe_freq_scale;
+
+unsigned long __ipipe_irq_tail;
+
+/*
+ * __ipipe_enable_pipeline() -- We are running on the boot CPU, hw
+ * interrupts are off, and secondary CPUs are still lost in space.
+ */
+void __ipipe_enable_pipeline(void)
+{
+	unsigned irq;
+
+	__ipipe_core_clock = get_cclk(); /* Fetch this once. */
+	__ipipe_freq_scale = 1000000000UL / __ipipe_core_clock;
+
+	for (irq = 0; irq < NR_IRQS; irq++) {
+		if (irq != IRQ_SW_INT1 && irq != IRQ_SW_INT2)
+			ipipe_virtualize_irq(ipipe_root_domain,
+					     irq,
+					     irq == IRQ_SYSTMR ?
+					     (ipipe_irq_handler_t)&__ipipe_do_timer :
+					     (ipipe_irq_handler_t)&__ipipe_do_IRQ,
+					     NULL,
+					     &__ipipe_ack_irq,
+					     IPIPE_HANDLE_MASK | IPIPE_PASS_MASK);
+	}
+}
+
+int __ipipe_ack_irq(unsigned irq)
+{
+	struct irqdesc *desc = irq_desc + irq;
+	unsigned long flags;
+	ipipe_declare_cpuid;
+
+	if (irq == IRQ_SYSTMR) {
+		/* Clear interrupt latch for TIMER0, don't mask. */
+		*pTIMER_STATUS = 1;
+		__builtin_bfin_ssync();
+		return 1;
+	}
+
+	/*
+	 * No need to mask IRQs at hw level: we are always called from
+	 * __ipipe_handle_irq(), so interrupts are already off. We
+	 * stall the pipeline so that spin_lock_irq*() ops won't
+	 * unintentionally flush it, since this could cause infinite
+	 * recursion.
+	 */
+
+	ipipe_load_cpuid();
+	flags = ipipe_test_and_stall_pipeline();
+	preempt_disable();
+	desc->chip->ack(irq);
+	preempt_enable_no_resched();
+	ipipe_restore_pipeline_nosync(ipipe_percpu_domain[cpuid], flags, cpuid);
+
+	return 1;
+}
+
+/*
+ * __ipipe_walk_pipeline(): Plays interrupts pending in the log. Must
+ * be called with local hw interrupts disabled.
+ */
+static inline void __ipipe_walk_pipeline(struct list_head *pos, int cpuid)
+{
+	struct ipipe_domain *this_domain = ipipe_percpu_domain[cpuid];
+	int s = -1;
+
+	if (test_bit(IPIPE_ROOTLOCK_FLAG, &ipipe_root_domain->flags))
+		s = __test_and_set_bit(IPIPE_STALL_FLAG,
+				       &ipipe_root_domain->cpudata[cpuid].status);
+
+	while (pos != &__ipipe_pipeline) {
+		struct ipipe_domain *next_domain =
+			list_entry(pos, struct ipipe_domain, p_link);
+
+		if (test_bit(IPIPE_STALL_FLAG,
+			     &next_domain->cpudata[cpuid].status))
+			break;	/* Stalled stage -- do not go further. */
+
+		if (next_domain->cpudata[cpuid].irq_pending_hi != 0) {
+
+			if (next_domain == this_domain)
+				__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+			else {
+				__ipipe_switch_to(this_domain, next_domain, cpuid);
+
+				ipipe_load_cpuid();	/* Processor might have changed. */
+
+				if (this_domain->cpudata[cpuid].irq_pending_hi != 0
+				    && !test_bit(IPIPE_STALL_FLAG,
+						 &this_domain->cpudata[cpuid].status))
+					__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+			}
+
+			break;
+		} else if (next_domain == this_domain)
+			break;
+
+		pos = next_domain->p_link.next;
+	}
+
+	if (!s)
+		__clear_bit(IPIPE_STALL_FLAG,
+			    &ipipe_root_domain->cpudata[cpuid].status);
+}
+
+/*
+ * __ipipe_handle_irq() -- IPIPE's generic IRQ handler. An optimistic
+ * interrupt protection log is maintained here for each domain. Hw
+ * interrupts are masked on entry.
+ */
+void __ipipe_handle_irq(unsigned irq, struct pt_regs *regs)
+{
+	struct ipipe_domain *this_domain;
+	struct list_head *head, *pos;
+	ipipe_declare_cpuid;
+	int m_ack, s_ack;
+
+	m_ack = (regs == NULL);	/* Software-triggered IRQs do not need
+				 * any ack. */
+	ipipe_load_cpuid();
+
+	this_domain = ipipe_percpu_domain[cpuid];
+
+	s_ack = m_ack;
+
+	if (test_bit(IPIPE_STICKY_FLAG, &this_domain->irqs[irq].control))
+		head = &this_domain->p_link;
+	else
+		head = __ipipe_pipeline.next;
+
+	/* Ack the interrupt. */
+
+	pos = head;
+
+	while (pos != &__ipipe_pipeline) {
+		struct ipipe_domain *next_domain =
+			list_entry(pos, struct ipipe_domain, p_link);
+
+		/*
+		 * For each domain handling the incoming IRQ, mark it as
+		 * pending in its log.
+		 */
+		if (test_bit(IPIPE_HANDLE_FLAG,
+			     &next_domain->irqs[irq].control)) {
+			/*
+			 * Domains that handle this IRQ are polled for
+			 * acknowledging it by decreasing priority order. The
+			 * interrupt must be made pending _first_ in the
+			 * domain's status flags before the PIC is unlocked.
+			 */
+
+			next_domain->cpudata[cpuid].irq_counters[irq].total_hits++;
+			next_domain->cpudata[cpuid].irq_counters[irq].pending_hits++;
+			__ipipe_set_irq_bit(next_domain, cpuid, irq);
+			ipipe_mark_irq_receipt(next_domain, irq, cpuid);
+
+			/*
+			 * Always get the first master acknowledge available.
+			 * Once we've got it, allow slave acknowledge
+			 * handlers to run (until one of them stops us).
+			 */
+			if (next_domain->irqs[irq].acknowledge != NULL) {
+				if (!m_ack)
+					m_ack = next_domain->irqs[irq].acknowledge(irq);
+				else if (test_bit
+					 (IPIPE_SHARED_FLAG,
+					  &next_domain->irqs[irq].control) && !s_ack)
+					s_ack = next_domain->irqs[irq].acknowledge(irq);
+			}
+		}
+
+		/*
+		 * If the domain does not want the IRQ to be passed down the
+		 * interrupt pipe, exit the loop now.
+		 */
+
+		if (!test_bit(IPIPE_PASS_FLAG, &next_domain->irqs[irq].control))
+			break;
+
+		pos = next_domain->p_link.next;
+	}
+
+	/*
+	 * Now walk the pipeline, yielding control to the highest
+	 * priority domain that has pending interrupt(s) or
+	 * immediately to the current domain if the interrupt has been
+	 * marked as 'sticky'. This search does not go beyond the
+	 * current domain in the pipeline.
+	 */
+	__ipipe_walk_pipeline(head, cpuid);
+}
+
+void __ipipe_do_IRQ(unsigned irq, struct pt_regs *regs)
+{
+	/* __ipipe_do_IRQ -> asm_do_IRQ -> do_edge/level_IRQ -> __do_irq -> action */
+	asm_do_IRQ(irq, regs);
+}
+
+void __ipipe_do_timer(unsigned irq, struct pt_regs *regs)
+{
+	timer_interrupt(irq,regs);
+}
+
+asmlinkage int __ipipe_check_root(void)
+{
+	ipipe_declare_cpuid;
+	/*
+	 * SMP: This routine is called with hw interrupts off, so no
+	 * migration can occur while checking the identity of the
+	 * current domain.
+	 */
+	ipipe_load_cpuid();
+	return ipipe_percpu_domain[cpuid] == ipipe_root_domain;
+}
+
+asmlinkage int __ipipe_syscall_root(struct pt_regs *regs)
+{
+	ipipe_declare_cpuid;
+	unsigned long flags;
+
+	/*
+	 * This routine either returns:
+	 * 0 -- if the syscall is to be passed to Linux;
+	 * 1 -- if the syscall should not be passed to Linux, and no
+	 * tail work should be performed;
+	 * -1 -- if the syscall should not be passed to Linux but the
+	 * tail work has to be performed (for handling signals etc).
+	 */
+
+	if (__ipipe_event_pipelined_p(IPIPE_EVENT_SYSCALL) &&
+	    __ipipe_dispatch_event(IPIPE_EVENT_SYSCALL,regs) > 0) {
+		/*
+		 * We might enter here over a non-root domain and exit
+		 * over the root one as a result of the syscall
+		 * (i.e. by recycling the register set of the current
+		 * context across the migration), so we need to fixup
+		 * the interrupt flag upon return too, so that
+		 * __ipipe_unstall_iret_root() resets the correct
+		 * stall bit on exit.
+		 */
+		if (ipipe_current_domain == ipipe_root_domain && !in_atomic()) {
+			/*
+			 * Sync pending VIRQs before _TIF_NEED_RESCHED
+			 * is tested.
+			 */
+			ipipe_lock_cpu(flags);
+			if ((ipipe_root_domain->cpudata[cpuid].irq_pending_hi & IPIPE_IRQMASK_VIRT) != 0)
+				__ipipe_sync_stage(IPIPE_IRQMASK_VIRT);
+			ipipe_unlock_cpu(flags);
+			return -1;
+		}
+		return 1;
+	}
+
+	return 0;
+}
+
+void ipipe_stall_root_raw(void)
+{
+	ipipe_declare_cpuid;
+
+	ipipe_load_cpuid();	/* hw IRQs are off on entry. */
+
+	__set_bit(IPIPE_STALL_FLAG,
+		  &ipipe_root_domain->cpudata[cpuid].status);
+
+	ipipe_mark_domain_stall(ipipe_root_domain, cpuid);
+
+	local_irq_enable_hw();
+}
+
+void ipipe_unstall_root_raw(void)
+{
+	ipipe_declare_cpuid;
+
+	local_irq_disable_hw();
+
+	ipipe_load_cpuid();
+
+	__clear_bit(IPIPE_STALL_FLAG,
+		    &ipipe_root_domain->cpudata[cpuid].status);
+
+	ipipe_mark_domain_unstall(ipipe_root_domain, cpuid);
+
+	local_irq_enable_hw();
+}
+
+EXPORT_SYMBOL(__ipipe_core_clock);
+EXPORT_SYMBOL(__ipipe_freq_scale);
+EXPORT_SYMBOL(__ipipe_irq_tail);
+EXPORT_SYMBOL(show_stack);
diff -uNrp 2.6.12/arch/bfinnommu/kernel/irqchip.c 2.6.12-ipipe/arch/bfinnommu/kernel/irqchip.c
--- 2.6.12/arch/bfinnommu/kernel/irqchip.c	2005-04-28 17:47:24.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/irqchip.c	2006-01-05 10:34:43.000000000 +0100
@@ -50,6 +50,85 @@ static LIST_HEAD(irq_pending);
 struct irqdesc irq_desc[NR_IRQS];
 
 extern int init_arch_irq(void);
+
+#ifdef CONFIG_IPIPE
+
+/* Implement a threaded interrupt model a la PREEMPT_RT on top of the
+   I-pipe, so that Linux device IRQ handlers cannot defer the
+   interrupt tail code for too long. */
+
+#include <linux/kthread.h>
+
+static int create_irq_threads;
+
+static int do_irqd(void * __desc)
+{
+	struct irqdesc *desc = __desc;
+	unsigned irq = desc - irq_desc;
+
+	current->flags |= PF_NOFREEZE;
+
+	ipipe_setscheduler_root(current,SCHED_FIFO,
+				50 + IVG7 - __ipipe_get_irq_priority(irq));
+
+	while (!kthread_should_stop()) {
+		down(&desc->thrsem);
+		desc->thrhandler(irq,&__ipipe_tick_regs[smp_processor_id()]);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+static void kick_irqd(unsigned irq, void *cookie)
+{
+	struct irqdesc *desc = irq_desc + irq;
+	up(&desc->thrsem);
+}
+
+static int start_irq_thread(unsigned irq, struct irqdesc *desc)
+{
+	if (desc->thread || !create_irq_threads /* || (irq != IRQ_SYSTMR && irq != 27) */)
+		return 0;
+
+	sema_init(&desc->thrsem,0);
+	desc->thread = kthread_create(do_irqd, desc, "IRQ %d", irq);
+
+	if (!desc->thread) {
+		printk(KERN_ERR "irqd: could not create IRQ thread %d!\n", irq);
+		return -ENOMEM;
+	}
+
+	wake_up_process(desc->thread);
+
+	desc->thrhandler = (void (*)(unsigned,struct pt_regs *))ipipe_root_domain->irqs[irq].handler;
+	ipipe_root_domain->irqs[irq].handler = &kick_irqd;
+
+	return 0;
+}
+
+int __init init_irqthreads(void)
+{
+	unsigned irq;
+
+	create_irq_threads = 1;
+
+	for (irq = 0; irq < NR_IRQS; irq++) {
+		struct irqdesc *desc = irq_desc + irq;
+		if (desc->action)
+		    start_irq_thread(irq, desc);
+	}
+	return 0;
+}
+
+#else /* CONFIG_IPIPE */
+
+int __init init_irqthreads(void)
+{
+    return 0;
+}
+
+#endif /* CONFIG_IPIPE */
+
 /*
  * Dummy mask/unmask handler
  */
@@ -322,7 +401,9 @@ do_edge_IRQ(unsigned int irq, struct irq
 	/*
 	 * Acknowledge and clear the IRQ, but don't mask it.
 	 */
+#ifndef CONFIG_IPIPE
 	desc->chip->ack(irq);
+#endif /* CONFIG_IPIPE */
 
 	/*
 	 * Mark the IRQ currently in progress.
@@ -364,7 +445,9 @@ do_edge_IRQ(unsigned int irq, struct irq
 	 */
 	desc->pending = 1;
 	desc->chip->mask(irq);
+#ifndef CONFIG_IPIPE
 	desc->chip->ack(irq);
+#endif /* CONFIG_IPIPE */
 }
 
 /*
@@ -381,7 +464,9 @@ do_level_IRQ(unsigned int irq, struct ir
 	/*
 	 * Acknowledge, clear _AND_ disable the interrupt.
 	 */
+#ifndef CONFIG_IPIPE
 	desc->chip->ack(irq);
+#endif /* CONFIG_IPIPE */
 
 	if (likely(!desc->disable_depth)) {
 		kstat_cpu(cpu).irqs[irq]++;
@@ -588,6 +673,10 @@ int setup_irq(unsigned int irq, struct i
 	 * The following block of code has to be executed atomically
 	 */
 	desc = irq_desc + irq;
+#ifdef CONFIG_IPIPE
+	if (start_irq_thread(irq, desc))
+	    return -ENOMEM;
+#endif /* CONFIG_IPIPE */
 	spin_lock_irqsave(&irq_controller_lock, flags);
 	p = &desc->action;
 	if ((old = *p) != NULL) {
diff -uNrp 2.6.12/arch/bfinnommu/kernel/process.c 2.6.12-ipipe/arch/bfinnommu/kernel/process.c
--- 2.6.12/arch/bfinnommu/kernel/process.c	2005-09-08 13:32:34.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/process.c	2005-12-28 12:10:17.000000000 +0100
@@ -32,11 +32,13 @@ inline static void default_idle(void)
 {
     while(1) {
         leds_switch(LED_OFF);
-        while (!need_resched())
+        while (!need_resched()) {
+	    ipipe_suspend_domain();
             __asm__("nop;\n\t \
                      nop;\n\t \
                      nop;\n\t \
                      idle;\n\t" : : : "cc");
+	}
         leds_switch(LED_ON);
         schedule();
     }
diff -uNrp 2.6.12/arch/bfinnommu/kernel/time.c 2.6.12-ipipe/arch/bfinnommu/kernel/time.c
--- 2.6.12/arch/bfinnommu/kernel/time.c	2005-09-08 08:50:50.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/time.c	2005-12-28 11:42:00.000000000 +0100
@@ -29,6 +29,7 @@ extern int setup_irq(unsigned int, struc
 inline static void do_leds(void);
 
 extern u_long get_cclk(void);
+extern u_long get_sclk(void);
 
 #define TIME_SCALE 100
 #define CLOCKS_PER_JIFFY (get_cclk() / HZ / TIME_SCALE)
@@ -107,6 +108,27 @@ static struct irqaction bfin_timer_irq =
  
 void time_sched_init(irqreturn_t (*timer_routine)(int, struct pt_regs *))
 {
+#ifdef CONFIG_IPIPE
+	/* Use builtin TIMER0 for time source instead of the high
+	 * priority core timer; this makes the latter available to
+	 * client RTOS running over the I-pipe. Xenomai will make good
+	 * use of this. */
+
+	/* Power down the core timer, just to play safe. */
+	*pTCNTL = 0;
+	__builtin_bfin_csync();
+	/* We use TIMER0 in PWM_OUT, periodic mode. */
+	*pTIMER_DISABLE = 1;	/* Disable TIMER0 for now. */
+	__builtin_bfin_ssync();
+	*pTIMER0_CONFIG = 0x19;	/* IRQ enable, periodic, PWM_OUT, SCLKed */
+	__builtin_bfin_ssync();
+	*pTIMER0_PERIOD = get_sclk() / HZ;
+	__builtin_bfin_ssync();
+	*pTIMER0_WIDTH = 1;
+	__builtin_bfin_ssync();
+	*pTIMER_ENABLE = 1;	/* Enable TIMER0. */
+	__builtin_bfin_ssync();
+#else /* !CONFIG_IPIPE */
 	/* power up the timer, but don't enable it just yet */
 	*pTCNTL = 1;
 	__builtin_bfin_csync();
@@ -122,9 +144,10 @@ void time_sched_init(irqreturn_t (*timer
 	__builtin_bfin_csync();
 	
 	*pTCNTL = 7;
-	
-	/* call setup_irq instead of request_irq because request_irq calls kmalloc which has not been initialized yet */
-	setup_irq(IRQ_CORETMR, &bfin_timer_irq);
+#endif /* CONFIG_IPIPE */
+	/* call setup_irq instead of request_irq because request_irq
+	   calls kmalloc which has not been initialized yet */
+	setup_irq(IRQ_SYSTMR, &bfin_timer_irq);
 }
 
 unsigned long gettimeoffset (void)
@@ -135,7 +158,7 @@ unsigned long gettimeoffset (void)
 	offset = tick_usec * (clocks_per_jiffy - (*pTCOUNT + 1)) / clocks_per_jiffy;
 
 	/* Check if we just wrapped the counters and maybe missed a tick */
-	if ((*pILAT & (1<<IRQ_CORETMR)) && (offset < (100000 / HZ / 2))){
+	if ((*pILAT & (1<<IRQ_PRIOTMR)) && (offset < (100000 / HZ / 2))){
 		
 		offset += (1000000 / HZ); 
 	} 
@@ -193,7 +216,7 @@ irqreturn_t timer_interrupt(int irq, str
 #ifndef CONFIG_SMP
 	local_irq_disable();        /* kernel requires irq_disabled during following function */
 	update_process_times(user_mode(regs));
-	local_irq_enable()
+	local_irq_enable();
 #endif
 	profile_tick(CPU_PROFILING, regs);
 
diff -uNrp 2.6.12/arch/bfinnommu/kernel/traps.c 2.6.12-ipipe/arch/bfinnommu/kernel/traps.c
--- 2.6.12/arch/bfinnommu/kernel/traps.c	2005-09-19 07:47:21.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/kernel/traps.c	2005-12-28 12:11:09.000000000 +0100
@@ -52,6 +52,7 @@
 asmlinkage void evt_system_call(void);
 asmlinkage void evt_soft_int1(void);
 asmlinkage void trap(void);
+asmlinkage void _deferred_ret_from_exception(void);
 
 extern void dump(struct pt_regs *fp);
 extern void _cplb_hdr(void);
@@ -182,6 +183,10 @@ asmlinkage void trap_c(struct pt_regs *f
 		sig = SIGTRAP;
 		break;
 	}
+
+	if (ipipe_trap_notify(fp->seqstat & 0x3f,fp))
+	    goto nsig;
+
 	info.si_signo = sig;
 	info.si_errno = 0;
 	info.si_addr = (void *) fp->pc;
diff -uNrp 2.6.12/arch/bfinnommu/mach-bf533/Kconfig 2.6.12-ipipe/arch/bfinnommu/mach-bf533/Kconfig
--- 2.6.12/arch/bfinnommu/mach-bf533/Kconfig	2005-08-19 14:19:30.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/mach-bf533/Kconfig	2005-12-28 11:43:24.000000000 +0100
@@ -60,7 +60,7 @@ config  DMA7_UARTTX
 
 config 	TIMER0
 	int "TIMER0"
-	default 11
+	default 7
 config 	TIMER1
 	int "TIMER1"
 	default 11
diff -uNrp 2.6.12/arch/bfinnommu/mach-common/entry.S 2.6.12-ipipe/arch/bfinnommu/mach-common/entry.S
--- 2.6.12/arch/bfinnommu/mach-common/entry.S	2005-09-15 15:24:25.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/mach-common/entry.S	2005-12-28 12:18:26.000000000 +0100
@@ -178,6 +178,7 @@ ENTRY(trap) /* Exception: 4th entry into
 badsys:
 	r7 = -ENOSYS; 		/* signextending enough */
 	[sp + PT_R0] = r7;	/* return value from system call */
+sysexit:
 	rti;	
 
 ENTRY(execve)
@@ -253,7 +254,19 @@ ENTRY(system_call)
 	p2 = [p2];
 
 	[p2+(TASK_THREAD+THREAD_KSP)] = sp;
-
+#ifdef CONFIG_IPIPE
+	r0 = sp; 
+	SP += -12;
+	call __ipipe_syscall_root; 
+	SP += 12;
+	cc = r0 == 1;
+	if cc jump syscall_really_exit;
+	cc = r0 == -1;
+	if cc jump resume_userspace;
+	r3 = [sp + PT_R3]; 
+	r4 = [sp + PT_R4]; 
+	p0 = [sp + PT_ORIG_P0]; 
+#endif /* CONFIG_IPIPE */
 	/* Check the System Call */
 	r7 = __NR_syscall; 
 	/*System call number is passed in P0 */
@@ -381,6 +394,18 @@ ENTRY(ret_from_exception)
 	csync;
 	r0 = [p2];
 	[sp + PT_IPEND] = r0;
+#ifdef CONFIG_IPIPE
+	[--sp] = rets;
+	[--sp] = r0;
+	SP += -12;
+	call __ipipe_check_root
+	SP += 12
+	r1 = r0;
+	r0 = [sp++];
+	rets = [sp++];
+	cc = r1 == 0;
+        if cc jump 4f;		/* not on behalf of Linux, get out */
+#endif /* CONFIG_IPIPE */
 
 1:
 	r1 = 0x17(Z);
@@ -425,6 +450,13 @@ ENTRY(ret_from_exception)
 ENTRY(_deferred_ret_from_exception)
 	SAVE_CONTEXT
         
+#ifdef CONFIG_IPIPE
+	SP += -12;
+	call __ipipe_check_root
+	SP += 12
+	cc = r0 == 0;
+        if cc jump 2f;		/* not on behalf of Linux, get out */
+#endif /* CONFIG_IPIPE */
 	r7 = sp;
 	r4.l = lo(ALIGN_PAGE_MASK);
 	r4.h = hi(ALIGN_PAGE_MASK);
diff -uNrp 2.6.12/arch/bfinnommu/mach-common/interrupt.S 2.6.12-ipipe/arch/bfinnommu/mach-common/interrupt.S
--- 2.6.12/arch/bfinnommu/mach-common/interrupt.S	2005-06-30 16:05:43.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/mach-common/interrupt.S	2005-12-28 12:18:58.000000000 +0100
@@ -1,3 +1,5 @@
+#define EXTRA_FRAME 200
+
 /* linux/arch/bfinnommu/mach-bf533/interrupt.S 
  *	 H/W interrupts
  *	 - assumes default interrupt configuration for the interrupts
@@ -22,6 +24,236 @@
 #include <asm/entry.h>
 #include <asm/asm-offsets.h>
 
+#ifdef CONFIG_IPIPE
+.macro handle_irq
+	r1 = sp
+	p2.l = lo(IPEND);
+        p2.h = hi(IPEND);
+        r2 = [p2];              /* Read current IPEND */
+        csync;
+        [sp + PT_IPEND] = r2;   /* Store IPEND */
+	sp += -12
+	call __ipipe_grab_irq
+	sp += 12
+	cc = r0 == 0;
+	if cc jump 1f;
+	call return_from_int;
+1:	
+.endm
+
+/* Create a 8-bytes backup area at the bottom of the interrupt
+   frame we will use to setup a deferred IRQ tail processing. */
+.macro	prepare_irq_entry
+	sp += -8
+.endm
+
+/* In case the current interrupt has preempted some kernel code, we
+   allow a deferred IRQ tail processing to be interposed
+   between the RTI and the preempted code resumption. Client RTOS
+   will need this to start their own rescheduling procedure _after_
+   the RTI has taken place, since they must not reschedule user-space
+   tasks from a nested supervisor event situation (i.e. kernel threads
+   already have bit 15 set in IPEND by construction, so one must not
+   reschedule directly from, e.g. a preempting core timer ISR for
+   which bit 6 is also set). Failing to enforce this rule would
+   wreck the internal logic of the IPEND register with respect
+   to the CPU operating states, which in turn would cause preempted
+   kernel threads to lose their supervisor privileges when resuming
+   from the original interrupt, and get an ILLRES exception after a
+   few context switches. */
+
+.macro 	prepare_irq_exit
+        p2.l = lo(IPEND);
+        p2.h = hi(IPEND);
+        csync;
+        r0 = [p2];
+        r1 = 1;
+        r1 = r0 - r1; /* if IPEND is a ^2, then a single event is */
+        r2 = r0 & r1; /* being processed, therefore RTI would return */
+        cc = r2 == 0 ; /* to user-space code. Otherwise, we did preempt */
+	if cc jump 1f ; /* kernel code. */
+        p0.l = __ipipe_irq_tail ;
+        p0.h = __ipipe_irq_tail ;
+	r2 = [p0] ; 
+	cc = r2 == 0 ;
+	if cc jump 1f ;
+	/* Save current IMASK to offset #0 of the backup area, and
+	clear interrupts at core level so that the IRQ tail processing
+	may not be preempted until it has been fully carried out, with
+	a RTI in the middle. */
+	cli r2
+	[sp + SIZEOF_PTREGS] = r2 ;
+	/* Save old RETI to offset #4 of the backup area, and change
+	the current interrupt return address so that we will branch to
+	the IRQ tail processing routine upon RTI. */
+        p2.l = __ipipe_irq_tail_start ;
+        p2.h = __ipipe_irq_tail_start ;
+	r2 = [sp + PT_PC] ; 
+	[sp + SIZEOF_PTREGS + 4] = r2 ; 
+	[sp + PT_PC] = p2 ;
+1:
+	/* Clearing PT_RESERVED means "no IRQ tail processing". */
+	[sp + PT_RESERVED] = r2 ; 
+.endm
+
+/* Fix up the stack frame in case we don't have any registered IRQ
+   tail processing routine. In such a case, we have to get rid of the
+   8-bytes backup area at the bottom of the interrupt frame. */
+
+.macro 	fixup_irq_frame
+	[--sp] = ASTAT ;
+	[--sp] = r0 ;
+	r0 = [sp - SIZEOF_PTREGS - PT_RESERVED - 8] ;
+	cc = r0 == 0 ;
+	r0 = [sp++]
+	if cc jump 1f ;
+	ASTAT = [sp++]
+	jump 2f
+1:
+	ASTAT = [sp++]
+	sp += 8
+2:
+.endm
+
+.macro save_tail_context
+	[--sp] = SYSCFG;
+	[--sp] = ( R7:0, P5:0 );
+	[--sp] = fp;
+	[--sp] = usp;
+	[--sp] = i0;
+	[--sp] = i1;
+	[--sp] = i2;
+	[--sp] = i3;
+	[--sp] = m0;
+	[--sp] = m1;
+	[--sp] = m2;
+	[--sp] = m3;
+	[--sp] = l0;
+	[--sp] = l1;
+	[--sp] = l2;
+	[--sp] = l3;
+	[--sp] = b0;
+	[--sp] = b1;
+	[--sp] = b2;
+	[--sp] = b3;
+	[--sp] = a0.x;
+	[--sp] = a0.w;
+	[--sp] = a1.x;
+	[--sp] = a1.w;
+	[--sp] = LC0;
+	[--sp] = LC1;
+	[--sp] = LT0;
+	[--sp] = LT1;
+	[--sp] = LB0;
+	[--sp] = LB1;
+	[--sp] = ASTAT;
+	[--sp] = RETS;
+	[--sp] = RETI;
+	[--sp] = RETX;
+	[--sp] = RETN;
+	[--sp] = RETE;
+	[--sp] = SEQSTAT;
+	/* Clear all L registers.  */
+	r0 = 0 (x);
+	l0 = r0;
+	l1 = r0;
+	l2 = r0;
+	l3 = r0;
+.endm
+
+.macro restore_tail_context
+	SEQSTAT = [sp++];
+	RETE = [sp++];
+	RETN = [sp++];
+	RETX = [sp++];
+	r0 = [sp++] /* Make sure not to mask interrupts */
+	RETI = r0   /* when restoring RETI. */
+	RETS = [sp++];
+	ASTAT = [sp++];
+	LB1 = [sp++];
+	LB0 = [sp++];
+	LT1 = [sp++];
+	LT0 = [sp++];
+	LC1 = [sp++];
+	LC0 = [sp++];
+	a1.w = [sp++];
+	a1.x = [sp++];
+	a0.w = [sp++];
+	a0.x = [sp++];
+	b3 = [sp++];
+	b2 = [sp++];
+	b1 = [sp++];
+	b0 = [sp++];
+	l3 = [sp++];
+	l2 = [sp++];
+	l1 = [sp++];
+	l0 = [sp++];
+	m3 = [sp++];
+	m2 = [sp++];
+	m1 = [sp++];
+	m0 = [sp++];
+	i3 = [sp++];
+	i2 = [sp++];
+	i1 = [sp++];
+	i0 = [sp++];
+	usp = [sp++];
+	fp = [sp++];
+	( R7 : 0, P5 : 0) = [ SP ++ ];
+	csync;
+	SYSCFG = [sp++];
+	csync;
+.endm
+
+/* Run the IRQ tail processing, then branch to the resumption
+   trampoline through an interrupt. */
+	
+ENTRY(__ipipe_irq_tail_start)
+
+	save_tail_context
+
+	p0.l = __ipipe_irq_tail ;
+        p0.h = __ipipe_irq_tail ;
+	p0 = [p0] ; 
+	sp += -12 ; 
+	call (p0) ;
+	sp += 12;
+
+	restore_tail_context
+
+	/* The exact sequence below _does matter_: decrement sp first,
+	raise, then unmask. */
+	
+	[--sp] = r0;
+	r0 = [sp + 4]
+	csync
+	raise 5;
+	sti r0;
+
+wait_loop:
+	jump wait_loop
+	
+/* This code branches back to the kernel code which has been preempted
+   by the original interrupt which led us here eventually. */
+
+ENTRY(__ipipe_irq_tail_end)
+
+	r0 = [sp++]	/* Restore R0 from __ipipe_irq_tail_start */
+	sp += 4		/* Skip IMask */
+	reti = [sp++]	/* Restore original PC */
+	rti;
+	
+#else /* !CONFIG_IPIPE */
+#define	prepare_irq_entry
+.macro handle_irq
+	r1 = sp
+	sp += -12
+	call do_irq
+	sp += 12
+.endm
+#define	prepare_irq_exit
+#define	fixup_irq_frame
+#endif /* CONFIG_IPIPE */
+	
 .text
 .align 4 	/* just in case */
 
@@ -67,23 +299,31 @@ ENTRY(evt_ivhw)
 
 /* interrupt routine for core timer - 6 */	 
 ENTRY(evt_timer)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
+	 r0 = IRQ_CORETMR;
+	 r1 = sp;	
+#ifdef CONFIG_IPIPE
+	 SP += -12;
+         call __ipipe_grab_timer;
+	 SP += 12;
+#else /* !CONFIG_IPIPE */
 	 p2.l = lo(IPEND);
          p2.h = hi(IPEND);
-         r0 = [p2];              /* Read current IPEND */
+         r2 = [p2];              /* Read current IPEND */
          csync;
-         [sp + PT_IPEND] = r0;   /* Store IPEND */
-
-	 r0 = IRQ_CORETMR;
-	 r1 = sp;	
-	 SP += -12;	
-     call timer_interrupt;
+         [sp + PT_IPEND] = r2;   /* Store IPEND */
+	 SP += -12;
+	 call timer_interrupt;
 	 SP += 12;
 	 call return_from_int;
+#endif /* CONFIG_IPIPE */
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
-	 rti; 
+	 fixup_irq_frame
+	 rti;
 
-/* interrupt routine for evt2 - 2 */	 
+/* interrupt routine for evt2 - 2 [Adeos: not pipelined] */
 ENTRY(evt_evt2)
 	 SAVE_ALL_SYS
 	 r0 = IRQ_NMI;
@@ -96,79 +336,79 @@ ENTRY(evt_evt2)
 
 /* interrupt routine for evt7 - 7 */	 
 ENTRY(evt_evt7)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
  	 r0 = EVT_IVG7_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
 /* interrupt routine for evt8 - 8 */
 ENTRY(evt_evt8)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG8_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
 /* interrupt routine for evt9 - 9 */	 
 ENTRY(evt_evt9)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG9_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
 /* interrupt routine for evt10	- 10 */	 
 ENTRY(evt_evt10)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG10_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
 /* interrupt routine for evt11	- 11 */	 
 ENTRY(evt_evt11)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG11_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
 /* interrupt routine for evt12	- 12 */	 
 ENTRY(evt_evt12)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG12_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
  /* interrupt routine for evt13	- 13 */
 ENTRY(evt_evt13)
+	 prepare_irq_entry
 	 SAVE_CONTEXT
 	 r0 = EVT_IVG13_P;
-	 r1 =  sp;
-	 SP += -12;	
-	 call do_irq;
-	 SP += 12;	
+	 handle_irq;
+	 prepare_irq_exit
 	 RESTORE_CONTEXT
+	 fixup_irq_frame
 	 rti; 
 
  /* interrupt routine for system_call - 14 */	 
diff -uNrp 2.6.12/arch/bfinnommu/mach-common/ints-priority-sc.c 2.6.12-ipipe/arch/bfinnommu/mach-common/ints-priority-sc.c
--- 2.6.12/arch/bfinnommu/mach-common/ints-priority-sc.c	2005-09-07 10:37:41.000000000 +0200
+++ 2.6.12-ipipe/arch/bfinnommu/mach-common/ints-priority-sc.c	2005-12-28 18:14:31.000000000 +0100
@@ -1,5 +1,5 @@
 /*
- * $Id: ints-priority-sc.c,v 1.7 2005/09/07 08:37:41 bernds Exp $ 
+ * $Id: ints-priority-sc.c,v 1.10 2005/12/28 17:14:31 rpm Exp $ 
  *
  *arch/bfinnommu/mach-common/ints-priority.c
  *
@@ -74,6 +74,9 @@ asmlinkage void evt_evt12(void);
 asmlinkage void evt_evt13(void);
 asmlinkage void evt_soft_int1(void);
 asmlinkage void evt_system_call(void);
+#ifdef CONFIG_IPIPE
+asmlinkage void __ipipe_irq_tail_end(void);
+#endif /* CONFIG_IPIPE */
 
 extern void program_IAR(void);
 static void search_IAR(void);	
@@ -108,33 +111,50 @@ static void __init search_IAR(void)	
 
 static void bf533_core_mask_irq(unsigned int irq)
 {
-	local_irq_disable();
+	local_irq_disable_hw();
 	irq_flags &= ~(1<<irq);
-	local_irq_enable();
+	local_irq_enable_hw();
 }
 
 static void bf533_core_unmask_irq(unsigned int irq)
 {
 	/* enable the interrupt */
-	local_irq_disable();
+	local_irq_disable_hw();
 	irq_flags |= 1<<irq;
-	local_irq_enable();
+	local_irq_enable_hw();
 	return;
 }
 
 static void bf533_internal_mask_irq(unsigned int irq)
 {
-  //dummy function
+#ifdef CONFIG_IPIPE
+	/* With Adeos on board, we do need to mask the interrupt as
+	 * part of our fast ack cycle in __ipipe_handle_irq(), so that
+	 * no interrupt storm can happen with level-triggered IRQs.
+	 * Remember that the actual IRQ handler execution may be
+	 * postponed until we exit from some undergoing Linux critical
+	 * section, so we must find a way to keep the interrupt line
+	 * quiet until then, at the latest. High priority domains
+	 * above Linux may handle this differently and earlier using
+	 * their own acknowledge routine passed to
+	 * ipipe_virtualize_irq(). */
+	unsigned long irq_mask;
+	local_irq_disable_hw();
+	irq_mask = (1<<(irq - (IRQ_CORETMR+1)));
+   	*pSIC_IMASK &= ~irq_mask;
+	__builtin_bfin_ssync();
+	local_irq_enable_hw();
+#endif /* CONFIG_IPIPE */
 }
 
 static void bf533_internal_unmask_irq(unsigned int irq)
 {
 	unsigned long irq_mask;
-	local_irq_disable();
+	local_irq_disable_hw();
 	irq_mask = (1<<(irq - (IRQ_CORETMR+1)));
    	*pSIC_IMASK |= irq_mask;
 	__builtin_bfin_ssync();
-	local_irq_enable();
+	local_irq_enable_hw();
 }
 
 static struct irqchip bf533_core_irqchip = {
@@ -163,19 +183,19 @@ static void bf537_generic_error_mask_irq
 
 	if(!error_int_mask) 
 		{
-			local_irq_disable();
+			local_irq_disable_hw();
 		   	*pSIC_IMASK |= (1<<(IRQ_GENERIC_ERROR - (IRQ_CORETMR+1)));
 			__builtin_bfin_ssync();
-			local_irq_enable();		
+			local_irq_enable_hw();		
 		}
 }
 
 static void bf537_generic_error_unmask_irq(unsigned int irq)
 {
-	local_irq_disable();
+	local_irq_disable_hw();
 	*pSIC_IMASK |= (1<<(IRQ_GENERIC_ERROR - (IRQ_CORETMR+1)));
 	__builtin_bfin_ssync();
-	local_irq_enable();
+	local_irq_enable_hw();
 
 	error_int_mask |= (1L << (irq - IRQ_PPI_ERROR));
 }
@@ -232,6 +252,10 @@ static void bf533_gpio_ack_irq(unsigned 
 	int gpionr = irq - IRQ_PF0;
 	int mask = (1L << gpionr);
 	*pFIO_FLAG_C = mask;
+#ifdef CONFIG_IPIPE
+	__builtin_bfin_ssync();
+	*pFIO_MASKB_C = mask;
+#endif /* CONFIG_IPIPE */
 //	if (gpio_edge_triggered & mask) {
 //		/* ack */
 //	} else {
@@ -355,7 +379,14 @@ int __init  init_arch_irq(void)
 #endif
 	*pEVT2  = evt_evt2;
 	*pEVT3	= trap;
+#ifdef CONFIG_IPIPE
+	/* For the time being, we hijack the IVHW interrupt for
+	   returning from our tail processing code. Not pretty,
+	   but... */
+	*pEVT5 = __ipipe_irq_tail_end;
+#else /* !CONFIG_IPIPE */
 	*pEVT5 	= evt_ivhw;
+#endif /* CONFIG_IPIPE */
 	*pEVT6 	= evt_timer;
 	*pEVT7 	= evt_evt7;
 	*pEVT8	= evt_evt8;
@@ -463,3 +494,83 @@ void do_irq(int vec, struct pt_regs *fp)
 
 	asm_do_IRQ(vec, fp);
 }
+
+#ifdef CONFIG_IPIPE
+
+int __ipipe_get_irq_priority(unsigned irq)
+{
+	int ient, prio;
+
+	if (irq <= IRQ_CORETMR)
+		return irq;
+
+	for (ient = 0; ient < NR_PERI_INTS; ient++) {
+		struct ivgx *ivg = ivg_table + ient;
+		if (ivg->irqno == irq) {
+			for (prio = 0; prio <= IVG13-IVG7; prio++) {
+				if (ivg7_13[prio].ifirst <= ivg &&
+				    ivg7_13[prio].istop > ivg) {
+					return prio + IVG7;
+				}
+			}
+		}
+	}
+
+	return IVG13;	/* Paranoïd. */
+}
+
+/* Hw interrupts are unmasked for higher priority levels on entry. */
+
+asmlinkage int __ipipe_grab_irq(int vec, struct pt_regs *regs)
+{
+	struct ivgx *ivg = ivg7_13[vec-IVG7].ifirst;
+	struct ivgx *ivg_stop = ivg7_13[vec-IVG7].istop;
+	unsigned long sic_status, flags;
+	ipipe_declare_cpuid;
+	int irq;
+
+	__builtin_bfin_ssync();
+ 	sic_status = *pSIC_IMASK & *pSIC_ISR;
+
+	for(;; ivg++) {
+		if (ivg >= ivg_stop)  {
+			num_spurious++;
+			return 0;
+		}
+		else if (sic_status & ivg->isrflag)
+			break;
+	}
+
+	irq = ivg->irqno;
+
+	local_irq_save_hw(flags);
+
+	ipipe_load_cpuid();
+
+	if (irq == IRQ_SYSTMR)
+	    /* for update_process_times() */
+	    __ipipe_tick_regs[cpuid].ipend = regs->ipend;
+
+	__ipipe_handle_irq(irq, regs);
+
+	local_irq_restore_hw(flags);
+
+	/* We check for a rescheduling opportunity upon every
+	   interrupt, so that IRQ threads can preempt quickly. */
+
+	return (ipipe_percpu_domain[cpuid] == ipipe_root_domain &&
+		!test_bit(IPIPE_STALL_FLAG,
+			  &ipipe_root_domain->cpudata[cpuid].status));
+}
+
+/* Hw interrupts are unmasked for higher priority levels on entry. */
+
+asmlinkage void __ipipe_grab_timer(unsigned irq, struct pt_regs *regs)
+{
+	unsigned long flags;
+	local_irq_save_hw(flags);
+	__ipipe_handle_irq(irq, regs);
+	local_irq_restore_hw(flags);
+}
+
+#endif /* CONFIG_IPIPE */
diff -uNrp 2.6.12/include/asm-bfinnommu/bitops.h 2.6.12-ipipe/include/asm-bfinnommu/bitops.h
--- 2.6.12/include/asm-bfinnommu/bitops.h	2005-08-12 04:43:39.000000000 +0200
+++ 2.6.12-ipipe/include/asm-bfinnommu/bitops.h	2005-12-28 11:45:15.000000000 +0100
@@ -87,9 +87,9 @@ static __inline__ void set_bit(int nr, v
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	local_irq_save_hw(flags);
 	*a |= mask;
-	local_irq_restore(flags);
+	local_irq_restore_hw(flags);
 }
 
 static __inline__ void __set_bit(int nr, volatile unsigned long * addr)
@@ -115,9 +115,9 @@ static __inline__ void clear_bit(int nr,
 	unsigned long flags;
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	local_irq_save_hw(flags);
 	*a &= ~mask;
-	local_irq_restore(flags);
+	local_irq_restore_hw(flags);
 }
 
 static __inline__ void __clear_bit(int nr, volatile unsigned long * addr)
@@ -137,9 +137,9 @@ static __inline__ void change_bit(int nr
 
 	ADDR += nr >> 5;
 	mask = 1 << (nr & 31);
-	local_irq_save(flags);
+	local_irq_save_hw(flags);
 	*ADDR ^= mask;
-	local_irq_restore(flags);
+	local_irq_restore_hw(flags);
 }
 
 static __inline__ void __change_bit(int nr, volatile unsigned long * addr)
@@ -160,10 +160,10 @@ static __inline__ int test_and_set_bit(i
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags); 
+	local_irq_save_hw(flags); 
 	retval = (mask & *a) != 0;
 	*a |= mask;
-	local_irq_restore(flags); 
+	local_irq_restore_hw(flags); 
 
 	return retval;
 }
@@ -188,10 +188,10 @@ static __inline__ int test_and_clear_bit
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	local_irq_save_hw(flags);
 	retval = (mask & *a) != 0;
 	*a &= ~mask;
-	local_irq_restore(flags);
+	local_irq_restore_hw(flags);
 
 	return retval;
 }
@@ -216,10 +216,10 @@ static __inline__ int test_and_change_bi
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	local_irq_save_hw(flags);
 	retval = (mask & *a) != 0;
 	*a ^= mask;
-	local_irq_restore(flags);
+	local_irq_restore_hw(flags);
 	return retval;
 }
 
diff -uNrp 2.6.12/include/asm-bfinnommu/ipipe.h 2.6.12-ipipe/include/asm-bfinnommu/ipipe.h
--- 2.6.12/include/asm-bfinnommu/ipipe.h	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/include/asm-bfinnommu/ipipe.h	2006-01-07 09:47:22.000000000 +0100
@@ -0,0 +1,184 @@
+/*   -*- linux-c -*-
+ *   include/asm-bfinnommu/ipipe.h
+ *
+ *   Copyright (C) 2002-2005 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __BFINNOMMU_IPIPE_H
+#define __NBINNOMMU_IPIPE_H
+
+#include <linux/config.h>
+
+#ifdef CONFIG_IPIPE
+
+#include <linux/cpumask.h>
+#include <linux/list.h>
+#include <linux/threads.h>
+#include <asm/ptrace.h>
+#include <asm/mach/irq.h>
+#include <asm/bitops.h>
+#include <asm/traps.h>
+#include <asm/irqchip.h>
+
+#define IPIPE_ARCH_STRING     "1.1-01"
+#define IPIPE_MAJOR_NUMBER    1
+#define IPIPE_MINOR_NUMBER    1
+#define IPIPE_PATCH_NUMBER    1
+
+#define IPIPE_NR_XIRQS		NR_IRQS
+#define IPIPE_IRQ_ISHIFT	5	/* 2^5 for 32bits arch. */
+
+/* Blackfin-specific, global domain flags */
+#define IPIPE_ROOTLOCK_FLAG	1	/* Lock pipeline for root */
+
+#ifdef CONFIG_SMP
+#error "I-pipe/bfinnommu: SMP not implemented"
+#else /* !CONFIG_SMP */
+#define ipipe_processor_id()	0
+#endif	/* CONFIG_SMP */
+
+#define prepare_arch_switch(rq, next)		\
+do { \
+	__ipipe_dispatch_event(IPIPE_EVENT_SCHEDULE,next);	\
+	local_irq_disable_hw();					\
+} while(0)
+
+#define task_running(rq, p)		((rq)->curr == (p))
+#define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
+
+#define task_hijacked(p)	\
+  ({ int x = ipipe_current_domain != ipipe_root_domain; \
+     __clear_bit(IPIPE_SYNC_FLAG,&ipipe_root_domain->cpudata[task_cpu(p)].status); \
+     local_irq_enable_hw(); x; })
+
+ /* Blackfin traps -- i.e. exception vector numbers */
+#define IPIPE_NR_FAULTS		52 /* We leave a gap after VEC_ILL_RES. */
+/* Pseudo-vectors used for kernel events */
+#define IPIPE_FIRST_EVENT	IPIPE_NR_FAULTS
+#define IPIPE_EVENT_SYSCALL	(IPIPE_FIRST_EVENT)
+#define IPIPE_EVENT_SCHEDULE	(IPIPE_FIRST_EVENT + 1)
+#define IPIPE_EVENT_SIGWAKE	(IPIPE_FIRST_EVENT + 2)
+#define IPIPE_EVENT_SETSCHED	(IPIPE_FIRST_EVENT + 3)
+#define IPIPE_EVENT_EXIT	(IPIPE_FIRST_EVENT + 4)
+#define IPIPE_LAST_EVENT	IPIPE_EVENT_EXIT
+#define IPIPE_NR_EVENTS		(IPIPE_LAST_EVENT + 1)
+
+struct ipipe_domain;
+
+struct ipipe_sysinfo {
+
+	int ncpus;		/* Number of CPUs on board */
+	u64 cpufreq;		/* CPU frequency (in Hz) */
+
+	/* Arch-dependent block */
+
+	struct {
+		unsigned tmirq;	/* Timer tick IRQ */
+		u64 tmfreq;	/* Timer frequency */
+	} archdep;
+};
+
+#define ipipe_read_tsc(t)					\
+	({							\
+	unsigned long __cy2;					\
+	__asm__ __volatile__ (	"1: %0 = CYCLES2\n"		\
+				"%1 = CYCLES\n"			\
+				"%2 = CYCLES2\n"		\
+				"CC = %2 == %0\n"		\
+				"if ! CC jump 1b\n"		\
+				:"=r" (((unsigned long *)&t)[1]),	\
+				"=r" (((unsigned long *)&t)[0]),	\
+				"=r" (__cy2)				\
+				: /*no input*/ : "CC");			\
+	t;								\
+	})
+
+#define ipipe_cpu_freq()	__ipipe_core_clock
+#define ipipe_tsc2ns(_t)	(((unsigned long)(_t)) * __ipipe_freq_scale)
+
+/* Private interface -- Internal use only */
+
+void ipipe_stall_root_raw(void);
+
+void ipipe_unstall_root_raw(void);
+
+#define __ipipe_check_platform()	do { } while(0)
+
+#define __ipipe_init_platform()		do { } while(0)
+
+#define __ipipe_enable_irq(irq)		irq_desc[irq].chip->unmask(irq)
+
+#define __ipipe_disable_irq(irq)	irq_desc[irq].chip->mask(irq)
+
+#define __ipipe_lock_root() \
+	set_bit(IPIPE_ROOTLOCK_FLAG, &ipipe_root_domain->flags)
+
+#define __ipipe_unlock_root() \
+	clear_bit(IPIPE_ROOTLOCK_FLAG, &ipipe_root_domain->flags)
+
+void __ipipe_enable_pipeline(void);
+
+void __ipipe_sync_stage(unsigned long syncmask);
+
+int __ipipe_ack_irq(unsigned irq);
+
+void __ipipe_do_IRQ(unsigned irq,
+		    struct pt_regs *regs);
+
+void __ipipe_do_timer(unsigned irq,
+		      struct pt_regs *regs);
+
+void __ipipe_handle_irq(unsigned irq,
+			struct pt_regs *regs);
+
+int __ipipe_get_irq_priority(unsigned irq);
+
+extern struct pt_regs __ipipe_tick_regs[];
+
+extern unsigned long __ipipe_core_clock;
+
+extern unsigned long __ipipe_freq_scale;
+
+extern unsigned long __ipipe_irq_tail;
+
+#define IPIPE_TIMER_IRQ  IRQ_CORETMR
+
+#define IRQ_SYSTMR     IRQ_TMR0
+
+#define IRQ_PRIOTMR    CONFIG_TIMER0
+
+unsigned long get_cclk(void);	/* Core clock freq (HZ) */
+
+unsigned long get_sclk(void);	/* System clock freq (HZ) */
+
+#else /* !CONFIG_IPIPE */
+
+#define ipipe_stall_root_raw()		do { } while(0)
+#define ipipe_unstall_root_raw()	do { } while(0)
+#define task_hijacked(p)		0
+#define ipipe_trap_notify(t,r)  	0
+
+#define IRQ_SYSTMR     IRQ_CORETMR
+
+#define IRQ_PRIOTMR    IRQ_CORETMR
+
+#endif /* CONFIG_IPIPE */
+
+int init_irqthreads(void);
+
+#endif	/* !__BFINNOMMU_IPIPE_H */
diff -uNrp 2.6.12/include/asm-bfinnommu/irqchip.h 2.6.12-ipipe/include/asm-bfinnommu/irqchip.h
--- 2.6.12/include/asm-bfinnommu/irqchip.h	2005-02-23 12:19:09.000000000 +0100
+++ 2.6.12-ipipe/include/asm-bfinnommu/irqchip.h	2005-12-28 11:45:28.000000000 +0100
@@ -14,6 +14,10 @@
 struct irqdesc;
 struct pt_regs;
 struct seq_file;
+#ifdef CONFIG_IPIPE
+#include <asm/semaphore.h>
+struct task_struct;
+#endif /* CONFIG_IPIPE */
 
 typedef void (*irq_handler_t)(unsigned int, struct irqdesc *, struct pt_regs *);
 typedef void (*irq_control_t)(unsigned int);
@@ -58,6 +62,11 @@ struct irqdesc {
 	void		*chipdata;
 	void		*data;
 	unsigned int	disable_depth;
+#ifdef CONFIG_IPIPE
+	struct task_struct *thread;
+	struct semaphore thrsem;
+	void (*thrhandler)(unsigned irq, struct pt_regs *);
+#endif /* CONFIG_IPIPE */
 
 	unsigned int	triggered: 1;		/* IRQ has occurred	      */
 	unsigned int	running  : 1;		/* IRQ is running             */
diff -uNrp 2.6.12/include/asm-bfinnommu/pgalloc.h 2.6.12-ipipe/include/asm-bfinnommu/pgalloc.h
--- 2.6.12/include/asm-bfinnommu/pgalloc.h	2005-01-17 13:28:24.000000000 +0100
+++ 2.6.12-ipipe/include/asm-bfinnommu/pgalloc.h	2005-12-28 11:45:34.000000000 +0100
@@ -5,4 +5,9 @@
 
 #define check_pgt_cache()	do { } while (0)
 
+static inline void set_pgdir(unsigned long address, pgd_t entry)
+{
+    /* nop */
+}
+
 #endif /* _BFINNOMMU_PGALLOC_H */
diff -uNrp 2.6.12/include/asm-bfinnommu/system.h 2.6.12-ipipe/include/asm-bfinnommu/system.h
--- 2.6.12/include/asm-bfinnommu/system.h	2005-06-13 10:44:27.000000000 +0200
+++ 2.6.12-ipipe/include/asm-bfinnommu/system.h	2005-12-28 11:45:39.000000000 +0100
@@ -46,6 +46,66 @@ asmlinkage void resume(void);
 
 extern volatile unsigned long irq_flags;
 			
+#ifdef CONFIG_IPIPE
+
+void __ipipe_stall_root(void);
+
+void __ipipe_unstall_root(void);
+
+unsigned long __ipipe_test_root(void);
+
+unsigned long __ipipe_test_and_stall_root(void);
+
+void __ipipe_restore_root(unsigned long flags);
+
+#define local_save_flags(x)				\
+do {							\
+    (x) = __ipipe_test_root()?0x1f:0;			\
+} while(0)
+#define local_irq_save(x)			\
+do {						\
+    (x) = __ipipe_test_and_stall_root()?0x1f:0;	\
+} while(0)
+#define local_irq_restore(x)	__ipipe_restore_root(x&1)
+#define local_irq_disable()	__ipipe_stall_root()
+#define local_irq_enable()	__ipipe_unstall_root()
+#define irqs_disabled()		__ipipe_test_root()
+
+#define local_irq_enable_hw() 		\
+do {					\
+	__asm__ __volatile__ (		\
+		"sti %0;"		\
+		::"d"(irq_flags));	\
+} while(0)
+
+#define local_irq_disable_hw() 		\
+do {					\
+	int _tmp_dummy;			\
+	__asm__ __volatile__ (		\
+		"cli %0;"		\
+		:"=d" (_tmp_dummy):);	\
+} while(0)
+
+#define local_irq_save_hw(x) 		\
+do {					\
+	__asm__ __volatile__ (          \
+		"cli %0;"		\
+		:"=d"(x):);         	\
+} while(0)
+
+#define local_save_flags_hw(x) asm volatile ("cli %0;sti %0;":"=d"(x):)
+
+#define local_irq_restore_hw(x) asm volatile ("sti %0;" ::"d"(x))
+
+#define	irqs_disabled_hw()		\
+({					\
+	unsigned long flags;		\
+	local_save_flags_hw(flags);	\
+	(flags == 0x1f);	\
+})
+
+#else /* !CONFIG_IPIPE */
+
 #define local_irq_enable() {		\
 	__asm__ __volatile__ (		\
 		"sti %0;"		\
@@ -82,6 +142,13 @@ if (irq_flags == 0) printk("Whoops\n");	
 	(flags  == 0x1f);	\
 })
 
+#define local_irq_save_hw(flags)	local_irq_save(flags)
+#define local_irq_restore_hw(flags)	local_irq_restore(flags)
+#define local_irq_enable_hw()		local_irq_enable()
+#define local_irq_disable_hw(flags)	local_irq_disable()
+
+#endif /* CONFIG_IPIPE */
+
 /*
  * Force strict CPU ordering.
  */
@@ -118,7 +185,7 @@ static inline unsigned long __xchg(unsig
   unsigned long tmp=0;
   unsigned long flags = 0;
 
-  local_irq_save(flags);
+  local_irq_save_hw(flags);
 
   switch (size) {
   case 1:
@@ -140,7 +207,7 @@ static inline unsigned long __xchg(unsig
     : "=&d" (tmp) : "d" (x), "m" (*__xg(ptr)) : "memory");
     break;
   }
-  local_irq_restore(flags);
+  local_irq_restore_hw(flags);
   return tmp;
 }
 
@@ -155,7 +222,7 @@ static inline unsigned long __cmpxchg(vo
   unsigned long tmp=0;
   unsigned long flags = 0;
 
-  local_irq_save(flags);
+  local_irq_save_hw(flags);
 
   switch (size) {
   case 1:
@@ -186,7 +253,7 @@ static inline unsigned long __cmpxchg(vo
     : "=&d" (tmp) : "d" (old), "d" (new), "m" (*__xg(ptr)) : "memory");
     break;
   }
-  local_irq_restore(flags);
+  local_irq_restore_hw(flags);
   return tmp;
 }
 
diff -uNrp 2.6.12/include/linux/hardirq.h 2.6.12-ipipe/include/linux/hardirq.h
--- 2.6.12/include/linux/hardirq.h	2005-08-15 11:13:56.000000000 +0200
+++ 2.6.12-ipipe/include/linux/hardirq.h	2005-12-28 11:46:42.000000000 +0100
@@ -86,8 +86,21 @@ extern void synchronize_irq(unsigned int
 # define synchronize_irq(irq)	barrier()
 #endif
 
+#ifdef CONFIG_IPIPE
+#define nmi_enter() \
+do { \
+    if (ipipe_current_domain == ipipe_root_domain) \
+	irq_enter(); \
+} while(0)
+#define nmi_exit() \
+do { \
+    if (ipipe_current_domain == ipipe_root_domain) \
+	sub_preempt_count(HARDIRQ_OFFSET); \
+} while(0)
+#else /* !CONFIG_IPIPE */
 #define nmi_enter()		irq_enter()
 #define nmi_exit()		sub_preempt_count(HARDIRQ_OFFSET)
+#endif /* CONFIG_IPIPE */
 
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
 static inline void account_user_vtime(struct task_struct *tsk)
diff -uNrp 2.6.12/include/linux/ipipe.h 2.6.12-ipipe/include/linux/ipipe.h
--- 2.6.12/include/linux/ipipe.h	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/include/linux/ipipe.h	2006-01-06 12:43:53.000000000 +0100
@@ -0,0 +1,792 @@
+/* -*- linux-c -*-
+ * include/linux/ipipe.h
+ *
+ * Copyright (C) 2002-2005 Philippe Gerum.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ * USA; either version 2 of the License, or (at your option) any later
+ * version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __LINUX_IPIPE_H
+#define __LINUX_IPIPE_H
+
+#include <linux/config.h>
+#include <linux/spinlock.h>
+#include <linux/cache.h>
+#include <asm/ipipe.h>
+
+#ifdef CONFIG_IPIPE
+
+#define IPIPE_VERSION_STRING	IPIPE_ARCH_STRING
+#define IPIPE_RELEASE_NUMBER	((IPIPE_MAJOR_NUMBER << 16) | \
+				 (IPIPE_MINOR_NUMBER <<  8) | \
+				 (IPIPE_PATCH_NUMBER))
+
+#ifndef BROKEN_BUILTIN_RETURN_ADDRESS
+#define __BUILTIN_RETURN_ADDRESS0 ((unsigned long)__builtin_return_address(0))
+#define __BUILTIN_RETURN_ADDRESS1 ((unsigned long)__builtin_return_address(1))
+#endif /* !BUILTIN_RETURN_ADDRESS */
+
+#define IPIPE_ROOT_PRIO		100
+#define IPIPE_ROOT_ID		0
+#define IPIPE_ROOT_NPTDKEYS	4	/* Must be <= BITS_PER_LONG */
+
+#define IPIPE_RESET_TIMER	0x1
+#define IPIPE_GRAB_TIMER	0x2
+
+/* Global domain flags */
+#define IPIPE_SPRINTK_FLAG	0	/* Synchronous printk() allowed */
+
+#define IPIPE_STALL_FLAG	0	/* Stalls a pipeline stage */
+#define IPIPE_SYNC_FLAG		1	/* The interrupt syncer is running for the domain */
+
+#define IPIPE_HANDLE_FLAG	0
+#define IPIPE_PASS_FLAG		1
+#define IPIPE_ENABLE_FLAG	2
+#define IPIPE_DYNAMIC_FLAG	IPIPE_HANDLE_FLAG
+#define IPIPE_STICKY_FLAG	3
+#define IPIPE_SYSTEM_FLAG	4
+#define IPIPE_LOCK_FLAG		5
+#define IPIPE_SHARED_FLAG	6
+#define IPIPE_EXCLUSIVE_FLAG	31	/* ipipe_catch_event() is the reason why. */
+
+#define IPIPE_HANDLE_MASK	(1 << IPIPE_HANDLE_FLAG)
+#define IPIPE_PASS_MASK		(1 << IPIPE_PASS_FLAG)
+#define IPIPE_ENABLE_MASK	(1 << IPIPE_ENABLE_FLAG)
+#define IPIPE_DYNAMIC_MASK	IPIPE_HANDLE_MASK
+#define IPIPE_EXCLUSIVE_MASK	(1 << IPIPE_EXCLUSIVE_FLAG)
+#define IPIPE_STICKY_MASK	(1 << IPIPE_STICKY_FLAG)
+#define IPIPE_SYSTEM_MASK	(1 << IPIPE_SYSTEM_FLAG)
+#define IPIPE_LOCK_MASK		(1 << IPIPE_LOCK_FLAG)
+#define IPIPE_SHARED_MASK	(1 << IPIPE_SHARED_FLAG)
+#define IPIPE_SYNC_MASK		(1 << IPIPE_SYNC_FLAG)
+
+#define IPIPE_DEFAULT_MASK	(IPIPE_HANDLE_MASK|IPIPE_PASS_MASK)
+#define IPIPE_STDROOT_MASK	(IPIPE_HANDLE_MASK|IPIPE_PASS_MASK|IPIPE_SYSTEM_MASK)
+
+#define IPIPE_EVENT_SELF        0x80000000
+
+/* Number of virtual IRQs */
+#define IPIPE_NR_VIRQS		BITS_PER_LONG
+/* First virtual IRQ # */
+#define IPIPE_VIRQ_BASE		(((IPIPE_NR_XIRQS + BITS_PER_LONG - 1) / BITS_PER_LONG) * BITS_PER_LONG)
+/* Total number of IRQ slots */
+#define IPIPE_NR_IRQS		(IPIPE_VIRQ_BASE + IPIPE_NR_VIRQS)
+/* Number of indirect words needed to map the whole IRQ space. */
+#define IPIPE_IRQ_IWORDS	((IPIPE_NR_IRQS + BITS_PER_LONG - 1) / BITS_PER_LONG)
+#define IPIPE_IRQ_IMASK		(BITS_PER_LONG - 1)
+#define IPIPE_IRQMASK_ANY	(~0L)
+#define IPIPE_IRQMASK_VIRT	(IPIPE_IRQMASK_ANY << (IPIPE_VIRQ_BASE / BITS_PER_LONG))
+
+#ifdef CONFIG_SMP
+
+#define IPIPE_NR_CPUS		NR_CPUS
+#define ipipe_declare_cpuid	int cpuid
+#define ipipe_load_cpuid()	do { \
+					(cpuid) = ipipe_processor_id();	\
+				} while(0)
+#define ipipe_lock_cpu(flags)	do { \
+					local_irq_save_hw(flags); \
+					(cpuid) = ipipe_processor_id(); \
+				} while(0)
+#define ipipe_unlock_cpu(flags)	local_irq_restore_hw(flags)
+#define ipipe_get_cpu(flags)	ipipe_lock_cpu(flags)
+#define ipipe_put_cpu(flags)	ipipe_unlock_cpu(flags)
+#define ipipe_current_domain	(ipipe_percpu_domain[ipipe_processor_id()])
+
+#else /* !CONFIG_SMP */
+
+#define IPIPE_NR_CPUS		1
+#define ipipe_declare_cpuid	const int cpuid = 0
+#define ipipe_load_cpuid()	do { } while(0)
+#define ipipe_lock_cpu(flags)	local_irq_save_hw(flags)
+#define ipipe_unlock_cpu(flags)	local_irq_restore_hw(flags)
+#define ipipe_get_cpu(flags)	do { (void)(flags); } while(0)
+#define ipipe_put_cpu(flags)	do { } while(0)
+#define ipipe_current_domain	(ipipe_percpu_domain[0])
+
+#endif /* CONFIG_SMP */
+
+#define ipipe_virtual_irq_p(irq)	((irq) >= IPIPE_VIRQ_BASE && \
+					 (irq) < IPIPE_NR_IRQS)
+
+typedef void (*ipipe_irq_handler_t)(unsigned irq,
+				    void *cookie);
+
+typedef int (*ipipe_irq_ackfn_t)(unsigned irq);
+
+#define IPIPE_SAME_HANDLER	((ipipe_irq_handler_t)(-1))
+
+struct ipipe_domain {
+
+	struct list_head p_link;	/* Link in pipeline */
+
+	struct ipcpudata {
+		unsigned long status;
+		unsigned long irq_pending_hi;
+		unsigned long irq_pending_lo[IPIPE_IRQ_IWORDS];
+		struct ipirqcnt {
+			unsigned long pending_hits;
+			unsigned long total_hits;
+		} irq_counters[IPIPE_NR_IRQS];
+	} ____cacheline_aligned_in_smp cpudata[IPIPE_NR_CPUS];
+
+	struct {
+		unsigned long control;
+		ipipe_irq_ackfn_t acknowledge;
+		ipipe_irq_handler_t handler;
+		void *cookie;
+	} ____cacheline_aligned irqs[IPIPE_NR_IRQS];
+
+	int (*evhand[IPIPE_NR_EVENTS])(unsigned event,
+				       struct ipipe_domain *from,
+				       void *data); /* Event handlers. */
+	unsigned long long evself;	/* Self-monitored event bits. */
+
+#ifdef CONFIG_IPIPE_STATS
+	struct ipipe_stats { /* All in timebase units. */
+		unsigned long long last_stall_date;
+		unsigned long last_stall_eip;
+		unsigned long max_stall_time;
+		unsigned long max_stall_eip;
+		struct ipipe_irq_stats {
+			unsigned long long last_receipt_date;
+			unsigned long max_delivery_time;
+		} irq_stats[IPIPE_NR_IRQS];
+	} ____cacheline_aligned_in_smp stats[IPIPE_NR_CPUS];
+#endif /* CONFIG_IPIPE_STATS */
+	unsigned long flags;
+	unsigned domid;
+	const char *name;
+	int priority;
+	void *pdd;
+};
+
+struct ipipe_domain_attr {
+
+	unsigned domid;		/* Domain identifier -- Magic value set by caller */
+	const char *name;	/* Domain name -- Warning: won't be dup'ed! */
+	int priority;		/* Priority in interrupt pipeline */
+	void (*entry) (void);	/* Domain entry point */
+	void *pdd;		/* Per-domain (opaque) data pointer */
+};
+
+/* The following macros must be used hw interrupts off. */
+
+#define __ipipe_irq_cookie(ipd,irq)	(ipd)->irqs[irq].cookie
+#define __ipipe_irq_handler(ipd,irq)	(ipd)->irqs[irq].handler
+
+#define __ipipe_cpudata_irq_hits(ipd,cpuid,irq)	((ipd)->cpudata[cpuid].irq_counters[irq].total_hits)
+
+#define __ipipe_set_irq_bit(ipd,cpuid,irq) \
+do { \
+	if (!test_bit(IPIPE_LOCK_FLAG,&(ipd)->irqs[irq].control)) { \
+		__set_bit(irq & IPIPE_IRQ_IMASK,&(ipd)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+		__set_bit(irq >> IPIPE_IRQ_ISHIFT,&(ipd)->cpudata[cpuid].irq_pending_hi); \
+	} \
+} while(0)
+
+#define __ipipe_clear_pend(ipd,cpuid,irq) \
+do { \
+	__clear_bit(irq & IPIPE_IRQ_IMASK,&(ipd)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+	if ((ipd)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT] == 0) \
+		__clear_bit(irq >> IPIPE_IRQ_ISHIFT,&(ipd)->cpudata[cpuid].irq_pending_hi); \
+} while(0)
+
+#define __ipipe_lock_irq(ipd,cpuid,irq) \
+do { \
+	if (!test_and_set_bit(IPIPE_LOCK_FLAG,&(ipd)->irqs[irq].control)) \
+		__ipipe_clear_pend(ipd,cpuid,irq); \
+} while(0)
+
+#define __ipipe_unlock_irq(ipd,irq) \
+do { \
+	int __cpuid, __nr_cpus = num_online_cpus(); \
+	if (test_and_clear_bit(IPIPE_LOCK_FLAG,&(ipd)->irqs[irq].control)) \
+		for (__cpuid = 0; __cpuid < __nr_cpus; __cpuid++) \
+			if ((ipd)->cpudata[__cpuid].irq_counters[irq].pending_hits > 0) { /* We need atomic ops next. */ \
+				set_bit(irq & IPIPE_IRQ_IMASK,&(ipd)->cpudata[__cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+				set_bit(irq >> IPIPE_IRQ_ISHIFT,&(ipd)->cpudata[__cpuid].irq_pending_hi); \
+			} \
+} while(0)
+
+#define __ipipe_clear_irq(ipd,irq) \
+do { \
+	int __cpuid, __nr_cpus = num_online_cpus(); \
+	clear_bit(IPIPE_LOCK_FLAG,&(ipd)->irqs[irq].control); \
+	for (__cpuid = 0; __cpuid < __nr_cpus; __cpuid++) { \
+		(ipd)->cpudata[__cpuid].irq_counters[irq].pending_hits = 0; \
+		__ipipe_clear_pend(ipd,__cpuid,irq); \
+	} \
+} while(0)
+
+#ifdef __RAW_SPIN_LOCK_UNLOCKED
+#define spin_lock_hw(x)			_raw_spin_lock(x)
+#define spin_trylock_hw(x)		_raw_spin_trylock(x)
+#define spin_unlock_hw(x)		_raw_spin_unlock(x)
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+#define write_lock_hw(x)		_raw_write_lock(x)
+#define write_trylock_hw(x)		_raw_write_trylock(x)
+#define write_unlock_hw(x)		_raw_write_unlock(x)
+#define read_lock_hw(x)			_raw_read_lock(x)
+#define read_trylock_hw(x)		_raw_read_trylock(x)
+#define read_unlock_hw(x)		_raw_read_unlock(x)
+#else /* UP non-debug */
+#define write_lock_hw(lock)		do { (void)(lock); } while (0)
+#define write_trylock_hw(lock)		({ (void)(lock); 1; })
+#define write_unlock_hw(lock)		do { (void)(lock); } while (0)
+#define read_lock_hw(lock)		do { (void)(lock); } while (0)
+#define read_trylock_hw(lock)		({ (void)(lock); 1; })
+#define read_unlock_hw(lock)		do { (void)(lock); } while (0)
+#endif /* CONFIG_SMP || CONFIG_DEBUG_SPINLOCK */
+#else	/* !__RAW_SPIN_LOCK_UNLOCKED */
+#define spin_lock_hw(x)			_spin_lock(x)
+#define spin_unlock_hw(x)		_spin_unlock(x)
+#define spin_trylock_hw(x)		_spin_trylock(x)
+#define write_lock_hw(x)		_write_lock(x)
+#define write_unlock_hw(x)		_write_unlock(x)
+#define write_trylock_hw(x)		_write_trylock(x)
+#define read_lock_hw(x)			_read_lock(x)
+#define read_unlock_hw(x)		_read_unlock(x)
+#endif	/* __RAW_SPIN_LOCK_UNLOCKED */
+
+typedef spinlock_t			ipipe_spinlock_t;
+typedef rwlock_t			ipipe_rwlock_t;
+#define IPIPE_SPIN_LOCK_UNLOCKED	SPIN_LOCK_UNLOCKED
+#define IPIPE_RW_LOCK_UNLOCKED		RW_LOCK_UNLOCKED
+
+#define spin_lock_irqsave_hw(x,flags)		\
+do {						\
+	local_irq_save_hw(flags);		\
+	spin_lock_hw(x);			\
+} while (0)
+
+#define spin_unlock_irqrestore_hw(x,flags)	\
+do {						\
+	spin_unlock_hw(x);			\
+	local_irq_restore_hw(flags);		\
+} while (0)
+
+#define spin_lock_irq_hw(x)			\
+do {						\
+	local_irq_disable_hw();			\
+	spin_lock_hw(x);			\
+} while (0)
+
+#define spin_unlock_irq_hw(x)			\
+do {						\
+	spin_unlock_hw(x);			\
+	local_irq_enable_hw();			\
+} while (0)
+
+#define read_lock_irqsave_hw(lock, flags)	\
+do {						\
+	local_irq_save_hw(flags);		\
+	read_lock_hw(lock);			\
+} while (0)
+
+#define read_unlock_irqrestore_hw(lock, flags)	\
+do {						\
+	read_unlock_hw(lock);			\
+	local_irq_restore_hw(flags);		\
+} while (0)
+
+#define write_lock_irqsave_hw(lock, flags)	\
+do {						\
+	local_irq_save_hw(flags);		\
+	write_lock_hw(lock);			\
+} while (0)
+
+#define write_unlock_irqrestore_hw(lock, flags)	\
+do {						\
+	write_unlock_hw(lock);			\
+	local_irq_restore_hw(flags);		\
+} while (0)
+
+extern struct ipipe_domain *ipipe_percpu_domain[], *ipipe_root_domain;
+
+extern unsigned __ipipe_printk_virq;
+
+extern unsigned long __ipipe_virtual_irq_map;
+
+extern struct list_head __ipipe_pipeline;
+
+extern ipipe_spinlock_t __ipipe_pipelock;
+
+extern int __ipipe_event_monitors[];
+
+/* Private interface */
+
+void ipipe_init(void);
+
+#ifdef CONFIG_PROC_FS
+void ipipe_init_proc(void);
+
+#ifdef CONFIG_IPIPE_TRACE
+void __ipipe_init_trace_proc(void);
+#else /* !CONFIG_IPIPE_TRACE */
+#define __ipipe_init_trace_proc()   do { } while(0)
+#endif /* CONFIG_IPIPE_TRACE */
+
+#else	/* !CONFIG_PROC_FS */
+#define ipipe_init_proc()	do { } while(0)
+#endif	/* CONFIG_PROC_FS */
+
+void __ipipe_init_stage(struct ipipe_domain *ipd);
+
+void __ipipe_cleanup_domain(struct ipipe_domain *ipd);
+
+void __ipipe_add_domain_proc(struct ipipe_domain *ipd);
+
+void __ipipe_remove_domain_proc(struct ipipe_domain *ipd);
+
+void __ipipe_flush_printk(unsigned irq, void *cookie);
+
+void __ipipe_stall_root(void);
+
+void __ipipe_unstall_root(void);
+
+unsigned long __ipipe_test_root(void);
+
+unsigned long __ipipe_test_and_stall_root(void);
+
+void fastcall __ipipe_restore_root(unsigned long flags);
+
+int fastcall __ipipe_schedule_irq(unsigned irq, struct list_head *head);
+
+int fastcall __ipipe_dispatch_event(unsigned event, void *data);
+
+#define __ipipe_pipeline_head_p(ipd) (&(ipd)->p_link == __ipipe_pipeline.next)
+
+#define __ipipe_event_pipelined_p(ev) \
+	(__ipipe_event_monitors[ev] > 0 || (ipipe_current_domain->evself & (1LL << ev)))
+
+#ifdef CONFIG_SMP
+
+cpumask_t __ipipe_set_irq_affinity(unsigned irq,
+				   cpumask_t cpumask);
+
+int fastcall __ipipe_send_ipi(unsigned ipi,
+			      cpumask_t cpumask);
+
+#endif /* CONFIG_SMP */
+
+/* Called with hw interrupts off. */
+static inline void __ipipe_switch_to(struct ipipe_domain *out,
+				     struct ipipe_domain *in, int cpuid)
+{
+	void ipipe_suspend_domain(void);
+
+	/*
+	 * "in" is guaranteed to be closer than "out" from the head of the
+	 * pipeline (and obviously different).
+	 */
+
+	ipipe_percpu_domain[cpuid] = in;
+
+	ipipe_suspend_domain();	/* Sync stage and propagate interrupts. */
+	ipipe_load_cpuid();	/* Processor might have changed. */
+
+	if (ipipe_percpu_domain[cpuid] == in)
+		/*
+		 * Otherwise, something has changed the current domain under
+		 * our feet recycling the register set; do not override.
+		 */
+		ipipe_percpu_domain[cpuid] = out;
+}
+
+static inline void ipipe_sigwake_notify(struct task_struct *p)
+{
+	if (__ipipe_event_pipelined_p(IPIPE_EVENT_SIGWAKE))
+		__ipipe_dispatch_event(IPIPE_EVENT_SIGWAKE,p);
+}
+
+static inline void ipipe_setsched_notify(struct task_struct *p)
+{
+	if (__ipipe_event_pipelined_p(IPIPE_EVENT_SETSCHED))
+		__ipipe_dispatch_event(IPIPE_EVENT_SETSCHED,p);
+}
+
+static inline void ipipe_exit_notify(struct task_struct *p)
+{
+	if (__ipipe_event_pipelined_p(IPIPE_EVENT_EXIT))
+		__ipipe_dispatch_event(IPIPE_EVENT_EXIT,p);
+}
+
+static inline int ipipe_trap_notify(int ex, struct pt_regs *regs)
+{
+	return __ipipe_event_pipelined_p(ex) ? __ipipe_dispatch_event(ex,regs) : 0;
+}
+
+#ifdef CONFIG_IPIPE_STATS
+
+#define ipipe_mark_domain_stall(ipd, cpuid)			\
+do {								\
+	__label__ here;						\
+	struct ipipe_stats *ips;				\
+here:								\
+	ips = (ipd)->stats + cpuid;				\
+	if (ips->last_stall_date == 0) {			\
+		ipipe_read_tsc(ips->last_stall_date);		\
+		ips->last_stall_eip = (unsigned long)&&here;	\
+	}							\
+} while(0)
+
+static inline void ipipe_mark_domain_unstall(struct ipipe_domain *ipd, int cpuid)
+{ /* Called w/ hw interrupts off. */
+	struct ipipe_stats *ips = ipd->stats + cpuid;
+	unsigned long long t, d;
+
+	if (ips->last_stall_date != 0) {
+		ipipe_read_tsc(t);
+		d = t - ips->last_stall_date;
+		if (d > ips->max_stall_time) {
+			ips->max_stall_time = d;
+			ips->max_stall_eip = ips->last_stall_eip;
+		}
+		ips->last_stall_date = 0;
+	}
+}
+
+static inline void ipipe_mark_irq_receipt(struct ipipe_domain *ipd, unsigned irq, int cpuid)
+{
+	struct ipipe_stats *ips = ipd->stats + cpuid;
+
+	if (ips->irq_stats[irq].last_receipt_date == 0) {
+		ipipe_read_tsc(ips->irq_stats[irq].last_receipt_date);
+	}
+}
+
+static inline void ipipe_mark_irq_delivery(struct ipipe_domain *ipd, unsigned irq, int cpuid)
+{ /* Called w/ hw interrupts off. */
+	struct ipipe_stats *ips = ipd->stats + cpuid;
+	unsigned long long t, d;
+
+	if (ips->irq_stats[irq].last_receipt_date != 0) {
+		ipipe_read_tsc(t);
+		d = t - ips->irq_stats[irq].last_receipt_date;
+		ips->irq_stats[irq].last_receipt_date = 0;
+		if (d > ips->irq_stats[irq].max_delivery_time)
+			ips->irq_stats[irq].max_delivery_time = d;
+	}
+}
+
+static inline void ipipe_reset_stats (void)
+{
+	int cpu, irq;
+	for_each_online_cpu(cpu) {
+		ipipe_root_domain->stats[cpu].last_stall_date = 0LL;
+		for (irq = 0; irq < IPIPE_NR_IRQS; irq++)
+			ipipe_root_domain->stats[cpu].irq_stats[irq].last_receipt_date = 0LL;
+	}
+}
+
+#else /* !CONFIG_IPIPE_STATS */
+
+#define ipipe_mark_domain_stall(ipd,cpuid)	do { } while(0)
+#define ipipe_mark_domain_unstall(ipd,cpuid)	do { } while(0)
+#define ipipe_mark_irq_receipt(ipd,irq,cpuid)	do { } while(0)
+#define ipipe_mark_irq_delivery(ipd,irq,cpuid)	do { } while(0)
+#define ipipe_reset_stats()			do { } while(0)
+
+#endif /* CONFIG_IPIPE_STATS */
+
+/* Public interface */
+
+int ipipe_register_domain(struct ipipe_domain *ipd,
+			  struct ipipe_domain_attr *attr);
+
+int ipipe_unregister_domain(struct ipipe_domain *ipd);
+
+void ipipe_suspend_domain(void);
+
+int ipipe_virtualize_irq(struct ipipe_domain *ipd,
+			 unsigned irq,
+			 ipipe_irq_handler_t handler,
+			 void *cookie,
+			 ipipe_irq_ackfn_t acknowledge,
+			 unsigned modemask);
+
+static inline int ipipe_share_irq(unsigned irq,
+				  ipipe_irq_ackfn_t acknowledge)
+{
+	return ipipe_virtualize_irq(ipipe_current_domain,
+				    irq,
+				    IPIPE_SAME_HANDLER,
+				    NULL,
+				    acknowledge,
+				    IPIPE_SHARED_MASK | IPIPE_HANDLE_MASK |
+				    IPIPE_PASS_MASK);
+}
+
+int ipipe_control_irq(unsigned irq,
+		      unsigned clrmask,
+		      unsigned setmask);
+
+unsigned ipipe_alloc_virq(void);
+
+int ipipe_free_virq(unsigned virq);
+
+int fastcall ipipe_trigger_irq(unsigned irq);
+
+static inline int ipipe_propagate_irq(unsigned irq)
+{
+
+	return __ipipe_schedule_irq(irq, ipipe_current_domain->p_link.next);
+}
+
+static inline int ipipe_schedule_irq(unsigned irq)
+{
+
+	return __ipipe_schedule_irq(irq, &ipipe_current_domain->p_link);
+}
+
+static inline void ipipe_stall_pipeline_from(struct ipipe_domain *ipd)
+{
+	ipipe_declare_cpuid;
+#ifdef CONFIG_SMP
+	unsigned long flags;
+
+	ipipe_lock_cpu(flags); /* Care for migration. */
+
+	__set_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_mark_domain_stall(ipd, cpuid);
+
+	if (!__ipipe_pipeline_head_p(ipd))
+		ipipe_unlock_cpu(flags);
+#else	/* CONFIG_SMP */
+	set_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_mark_domain_stall(ipd, cpuid);
+
+	if (__ipipe_pipeline_head_p(ipd))
+		local_irq_disable_hw();
+#endif	/* CONFIG_SMP */
+}
+
+static inline unsigned long ipipe_test_pipeline_from(struct ipipe_domain *ipd)
+{
+	unsigned long flags, s;
+	ipipe_declare_cpuid;
+
+	ipipe_get_cpu(flags);
+	s = test_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_put_cpu(flags);
+
+	return s;
+}
+
+static inline unsigned long ipipe_test_and_stall_pipeline_from(struct
+							       ipipe_domain
+							       *ipd)
+{
+	ipipe_declare_cpuid;
+	unsigned long s;
+#ifdef CONFIG_SMP
+	unsigned long flags;
+
+	ipipe_lock_cpu(flags); /* Care for migration. */
+
+	s = __test_and_set_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_mark_domain_stall(ipd, cpuid);
+
+	if (!__ipipe_pipeline_head_p(ipd))
+		ipipe_unlock_cpu(flags);
+#else	/* CONFIG_SMP */
+	s = test_and_set_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_mark_domain_stall(ipd, cpuid);
+
+	if (__ipipe_pipeline_head_p(ipd))
+		local_irq_disable_hw();
+#endif	/* CONFIG_SMP */
+
+	return s;
+}
+
+void fastcall ipipe_unstall_pipeline_from(struct ipipe_domain *ipd);
+
+static inline unsigned long ipipe_test_and_unstall_pipeline_from(struct
+								 ipipe_domain
+								 *ipd)
+{
+	unsigned long flags, s;
+	ipipe_declare_cpuid;
+
+	ipipe_get_cpu(flags);
+	s = test_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+	ipipe_unstall_pipeline_from(ipd);
+	ipipe_put_cpu(flags);
+
+	return s;
+}
+
+static inline void ipipe_unstall_pipeline(void)
+{
+	ipipe_unstall_pipeline_from(ipipe_current_domain);
+}
+
+static inline unsigned long ipipe_test_and_unstall_pipeline(void)
+{
+	return ipipe_test_and_unstall_pipeline_from(ipipe_current_domain);
+}
+
+static inline unsigned long ipipe_test_pipeline(void)
+{
+	return ipipe_test_pipeline_from(ipipe_current_domain);
+}
+
+static inline unsigned long ipipe_test_and_stall_pipeline(void)
+{
+	return ipipe_test_and_stall_pipeline_from(ipipe_current_domain);
+}
+
+static inline void ipipe_restore_pipeline_from(struct ipipe_domain *ipd,
+					       unsigned long flags)
+{
+	if (flags)
+		ipipe_stall_pipeline_from(ipd);
+	else
+		ipipe_unstall_pipeline_from(ipd);
+}
+
+static inline void ipipe_stall_pipeline(void)
+{
+	ipipe_stall_pipeline_from(ipipe_current_domain);
+}
+
+static inline void ipipe_restore_pipeline(unsigned long flags)
+{
+	ipipe_restore_pipeline_from(ipipe_current_domain, flags);
+}
+
+static inline void ipipe_restore_pipeline_nosync(struct ipipe_domain *ipd,
+						 unsigned long flags, int cpuid)
+{
+	/*
+	 * If cpuid is current, then it must be held on entry
+	 * (ipipe_get_cpu/local_irq_save_hw/local_irq_disable_hw).
+	 */
+
+	if (flags) {
+		__set_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+		ipipe_mark_domain_stall(ipd,cpuid);
+	}
+	else {
+		__clear_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+		ipipe_mark_domain_unstall(ipd,cpuid);
+	}
+}
+
+void ipipe_init_attr(struct ipipe_domain_attr *attr);
+
+int ipipe_get_sysinfo(struct ipipe_sysinfo *sysinfo);
+
+int ipipe_tune_timer(unsigned long ns,
+		     int flags);
+
+unsigned long ipipe_critical_enter(void (*syncfn) (void));
+
+void ipipe_critical_exit(unsigned long flags);
+
+static inline void ipipe_set_printk_sync(struct ipipe_domain *ipd)
+{
+	set_bit(IPIPE_SPRINTK_FLAG, &ipd->flags);
+}
+
+static inline void ipipe_set_printk_async(struct ipipe_domain *ipd)
+{
+	clear_bit(IPIPE_SPRINTK_FLAG, &ipd->flags);
+}
+
+int ipipe_catch_event(struct ipipe_domain *ipd,
+		      unsigned event,
+		      int (*handler)(unsigned event,
+				     struct ipipe_domain *ipd,
+				     void *data));
+
+cpumask_t ipipe_set_irq_affinity(unsigned irq,
+				 cpumask_t cpumask);
+
+int fastcall ipipe_send_ipi(unsigned ipi,
+			    cpumask_t cpumask);
+
+int ipipe_setscheduler_root(struct task_struct *p,
+			    int policy,
+			    int prio);
+
+int ipipe_reenter_root(struct task_struct *prev,
+		       int policy,
+		       int prio);
+
+int ipipe_alloc_ptdkey(void);
+
+int ipipe_free_ptdkey(int key);
+
+int fastcall ipipe_set_ptd(int key,
+			   void *value);
+
+void fastcall *ipipe_get_ptd(int key);
+
+#define local_irq_enable_hw_cond()		local_irq_enable_hw()
+#define local_irq_disable_hw_cond()		local_irq_disable_hw()
+#define local_irq_save_hw_cond(flags)		local_irq_save_hw(flags)
+#define local_irq_restore_hw_cond(flags)	local_irq_restore_hw(flags)
+#define spin_lock_irqsave_hw_cond(lock,flags)	spin_lock_irqsave_hw(lock,flags)
+#define spin_unlock_irqrestore_hw_cond(lock,flags) spin_unlock_irqrestore_hw(lock,flags)
+
+#define ipipe_irq_lock(irq)						\
+	do {								\
+		ipipe_declare_cpuid;					\
+		ipipe_load_cpuid();					\
+		__ipipe_lock_irq(ipipe_percpu_domain[cpuid], cpuid, irq);\
+	} while(0)
+
+#define ipipe_irq_unlock(irq)						\
+	do {								\
+		ipipe_declare_cpuid;					\
+		ipipe_load_cpuid();					\
+		__ipipe_unlock_irq(ipipe_percpu_domain[cpuid], irq);	\
+	} while(0)
+
+#else	/* !CONFIG_IPIPE */
+
+#define ipipe_init()				do { } while(0)
+#define ipipe_suspend_domain()			do { } while(0)
+#define ipipe_sigwake_notify(p)			do { } while(0)
+#define ipipe_setsched_notify(p)		do { } while(0)
+#define ipipe_exit_notify(p)			do { } while(0)
+#define ipipe_init_proc()			do { } while(0)
+#define ipipe_reset_stats()			do { } while(0)
+#define ipipe_trap_notify(t,r)			0
+
+#define spin_lock_hw(lock)			spin_lock(lock)
+#define spin_unlock_hw(lock)			spin_unlock(lock)
+#define spin_lock_irq_hw(lock)			spin_lock_irq(lock)
+#define spin_unlock_irq_hw(lock)		spin_unlock_irq(lock)
+#define spin_lock_irqsave_hw(lock,flags)	spin_lock_irqsave(lock, flags)
+#define spin_unlock_irqrestore_hw(lock,flags)	spin_unlock_irqrestore(lock, flags)
+
+#define local_irq_enable_hw_cond()		do { } while(0)
+#define local_irq_disable_hw_cond()		do { } while(0)
+#define local_irq_save_hw_cond(flags)		do { (void)(flags); } while(0)
+#define local_irq_restore_hw_cond(flags)	do { } while(0)
+#define spin_lock_irqsave_hw_cond(lock,flags)	do { (void)(flags); spin_lock(lock); } while(0)
+#define spin_unlock_irqrestore_hw_cond(lock,flags)	spin_unlock(lock)
+
+#define ipipe_irq_lock(irq)			do { } while(0)
+#define ipipe_irq_unlock(irq)			do { } while(0)
+
+#endif	/* CONFIG_IPIPE */
+
+#endif	/* !__LINUX_IPIPE_H */
diff -uNrp 2.6.12/include/linux/preempt.h 2.6.12-ipipe/include/linux/preempt.h
--- 2.6.12/include/linux/preempt.h	2005-08-12 05:36:44.000000000 +0200
+++ 2.6.12-ipipe/include/linux/preempt.h	2005-12-28 11:47:04.000000000 +0100
@@ -26,22 +26,39 @@
 
 asmlinkage void preempt_schedule(void);
 
-#define preempt_disable() \
-do { \
-	inc_preempt_count(); \
-	barrier(); \
+#ifdef CONFIG_IPIPE
+
+#include <asm/ipipe.h>
+
+extern struct ipipe_domain *ipipe_percpu_domain[], *ipipe_root_domain;
+
+#define ipipe_preempt_guard()	(ipipe_percpu_domain[ipipe_processor_id()] == ipipe_root_domain)
+#else
+#define ipipe_preempt_guard()	1
+#endif
+
+#define preempt_disable()						\
+do {									\
+	if (ipipe_preempt_guard()) {					\
+		inc_preempt_count();					\
+		barrier();						\
+	}								\
 } while (0)
 
-#define preempt_enable_no_resched() \
-do { \
-	barrier(); \
-	dec_preempt_count(); \
+#define preempt_enable_no_resched()					\
+do {									\
+	if (ipipe_preempt_guard()) {					\
+		barrier();						\
+		dec_preempt_count();					\
+	}								\
 } while (0)
 
-#define preempt_check_resched() \
-do { \
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
-		preempt_schedule(); \
+#define preempt_check_resched()						\
+do {									\
+	if (ipipe_preempt_guard()) {					\
+		if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))	\
+			preempt_schedule();				\
+	}								\
 } while (0)
 
 #define preempt_enable() \
diff -uNrp 2.6.12/include/linux/sched.h 2.6.12-ipipe/include/linux/sched.h
--- 2.6.12/include/linux/sched.h	2005-08-12 05:36:44.000000000 +0200
+++ 2.6.12-ipipe/include/linux/sched.h	2005-12-28 11:47:10.000000000 +0100
@@ -4,6 +4,7 @@
 #include <asm/param.h>	/* for HZ */
 
 #include <linux/config.h>
+#include <linux/ipipe.h>
 #include <linux/capability.h>
 #include <linux/threads.h>
 #include <linux/kernel.h>
@@ -740,6 +741,9 @@ struct task_struct {
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;
 #endif
+#ifdef CONFIG_IPIPE
+        void *ptd[IPIPE_ROOT_NPTDKEYS];
+#endif
 };
 
 static inline pid_t process_group(struct task_struct *tsk)
diff -uNrp 2.6.12/init/Kconfig 2.6.12-ipipe/init/Kconfig
--- 2.6.12/init/Kconfig	2005-08-16 08:39:58.000000000 +0200
+++ 2.6.12-ipipe/init/Kconfig	2006-01-06 17:27:31.000000000 +0100
@@ -69,6 +69,7 @@ menu "General setup"
 
 config LOCALVERSION
 	string "Local version - append to kernel release"
+	default "-ipipe"
 	help
 	  Append an extra string to the end of your kernel version.
 	  This will show up when you type uname, for example.
diff -uNrp 2.6.12/init/main.c 2.6.12-ipipe/init/main.c
--- 2.6.12/init/main.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/init/main.c	2005-12-28 11:47:54.000000000 +0100
@@ -383,6 +383,7 @@ static void noinline rest_init(void)
 	numa_default_policy();
 	unlock_kernel();
 	preempt_enable_no_resched();
+	ipipe_reset_stats();
 	cpu_idle();
 } 
 
@@ -468,6 +469,11 @@ asmlinkage void __init start_kernel(void
 	init_timers();
 	softirq_init();
 	time_init();
+	/*
+	 * We need to wait for the interrupt and time subsystems to be
+	 * initialized before enabling the pipeline.
+	 */
+ 	ipipe_init();
 
 	/*
 	 * HACK ALERT! This is early. We're enabling the console before
@@ -590,6 +596,7 @@ static void __init do_basic_setup(void)
 #ifdef CONFIG_SYSCTL
 	sysctl_init();
 #endif
+	ipipe_init_proc();
 
 	/* Networking initialization needs a process context */ 
 	sock_init();
@@ -653,6 +660,8 @@ static int init(void * unused)
 	/* Sets up cpus_possible() */
 	smp_prepare_cpus(max_cpus);
 
+	init_irqthreads();
+
 	do_pre_smp_initcalls();
 
 	fixup_cpu_present_map();
diff -uNrp 2.6.12/kernel/Makefile 2.6.12-ipipe/kernel/Makefile
--- 2.6.12/kernel/Makefile	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/Makefile	2006-01-06 17:27:32.000000000 +0100
@@ -28,6 +28,7 @@ obj-$(CONFIG_KPROBES) += kprobes.o
 obj-$(CONFIG_SYSFS) += ksysfs.o
 obj-$(CONFIG_GENERIC_HARDIRQS) += irq/
 obj-$(CONFIG_SECCOMP) += seccomp.o
+obj-$(CONFIG_IPIPE) += ipipe/
 
 ifneq ($(CONFIG_SCHED_NO_NO_OMIT_FRAME_POINTER),y)
 # According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
diff -uNrp 2.6.12/kernel/exit.c 2.6.12-ipipe/kernel/exit.c
--- 2.6.12/kernel/exit.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/exit.c	2005-12-28 11:48:21.000000000 +0100
@@ -813,6 +813,7 @@ fastcall NORET_TYPE void do_exit(long co
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead)
 		acct_process(code);
+ 	ipipe_exit_notify(tsk);
 	exit_mm(tsk);
 
 	exit_sem(tsk);
diff -uNrp 2.6.12/kernel/fork.c 2.6.12-ipipe/kernel/fork.c
--- 2.6.12/kernel/fork.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/fork.c	2005-12-28 11:48:25.000000000 +0100
@@ -1103,6 +1103,14 @@ static task_t *copy_process(unsigned lon
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);
 	retval = 0;
+#ifdef CONFIG_IPIPE
+	{
+	int k;
+
+	for (k = 0; k < IPIPE_ROOT_NPTDKEYS; k++)
+	    p->ptd[k] = NULL;
+	}
+#endif /* CONFIG_IPIPE */
 
 fork_out:
 	if (retval)
diff -uNrp 2.6.12/kernel/ipipe/Kconfig 2.6.12-ipipe/kernel/ipipe/Kconfig
--- 2.6.12/kernel/ipipe/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/kernel/ipipe/Kconfig	2005-12-30 16:26:19.000000000 +0100
@@ -0,0 +1,15 @@
+config IPIPE
+	bool "Interrupt pipeline"
+	default y
+	---help---
+	  Activate this option if you want the interrupt pipeline to be
+	  compiled in.
+
+config IPIPE_STATS
+	bool "Collect statistics"
+	depends on IPIPE
+	default n
+	---help---
+	  Activate this option if you want runtime statistics to be collected
+	  while the I-pipe is operating. This option adds a small overhead, but
+	  is useful to detect unexpected latency spots.
diff -uNrp 2.6.12/kernel/ipipe/Makefile 2.6.12-ipipe/kernel/ipipe/Makefile
--- 2.6.12/kernel/ipipe/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/kernel/ipipe/Makefile	2005-12-29 14:04:04.000000000 +0100
@@ -0,0 +1,3 @@
+
+obj-$(CONFIG_IPIPE)	+= core.o generic.o
+obj-$(CONFIG_IPIPE_TRACE) += tracer.o
diff -uNrp 2.6.12/kernel/ipipe/core.c 2.6.12-ipipe/kernel/ipipe/core.c
--- 2.6.12/kernel/ipipe/core.c	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/kernel/ipipe/core.c	2006-01-06 09:28:39.000000000 +0100
@@ -0,0 +1,827 @@
+/* -*- linux-c -*-
+ * linux/kernel/ipipe/core.c
+ *
+ * Copyright (C) 2002-2005 Philippe Gerum.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ * USA; either version 2 of the License, or (at your option) any later
+ * version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Architecture-independent I-PIPE core support.
+ */
+
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/interrupt.h>
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#endif	/* CONFIG_PROC_FS */
+
+static struct ipipe_domain ipipe_root =
+	{ .cpudata = {[0 ... IPIPE_NR_CPUS-1] =
+		{ .status = (1<<IPIPE_STALL_FLAG) } } };
+
+struct ipipe_domain *ipipe_root_domain = &ipipe_root;
+
+struct ipipe_domain *ipipe_percpu_domain[IPIPE_NR_CPUS] =
+	{[0 ... IPIPE_NR_CPUS - 1] = &ipipe_root };
+
+ipipe_spinlock_t __ipipe_pipelock = IPIPE_SPIN_LOCK_UNLOCKED;
+
+struct list_head __ipipe_pipeline;
+
+unsigned long __ipipe_virtual_irq_map = 0;
+
+#ifdef CONFIG_PRINTK
+unsigned __ipipe_printk_virq;
+#endif /* CONFIG_PRINTK */
+
+int __ipipe_event_monitors[IPIPE_NR_EVENTS];
+
+/*
+ * ipipe_init() -- Initialization routine of the IPIPE layer. Called
+ * by the host kernel early during the boot procedure.
+ */
+void ipipe_init(void)
+{
+	struct ipipe_domain *ipd = &ipipe_root;
+
+	__ipipe_check_platform();	/* Do platform dependent checks first. */
+
+	/*
+	 * A lightweight registration code for the root domain. We are
+	 * running on the boot CPU, hw interrupts are off, and
+	 * secondary CPUs are still lost in space.
+	 */
+
+	INIT_LIST_HEAD(&__ipipe_pipeline);
+
+	ipd->name = "Linux";
+	ipd->domid = IPIPE_ROOT_ID;
+	ipd->priority = IPIPE_ROOT_PRIO;
+
+	__ipipe_init_stage(ipd);
+
+	INIT_LIST_HEAD(&ipd->p_link);
+	list_add_tail(&ipd->p_link, &__ipipe_pipeline);
+
+	__ipipe_init_platform();
+
+#ifdef CONFIG_PRINTK
+	__ipipe_printk_virq = ipipe_alloc_virq();	/* Cannot fail here. */
+	ipd->irqs[__ipipe_printk_virq].handler = &__ipipe_flush_printk;
+	ipd->irqs[__ipipe_printk_virq].cookie = NULL;
+	ipd->irqs[__ipipe_printk_virq].acknowledge = NULL;
+	ipd->irqs[__ipipe_printk_virq].control = IPIPE_HANDLE_MASK;
+#endif /* CONFIG_PRINTK */
+
+	__ipipe_enable_pipeline();
+
+	printk(KERN_INFO "I-pipe %s: pipeline enabled.\n",
+	       IPIPE_VERSION_STRING);
+}
+
+void __ipipe_init_stage(struct ipipe_domain *ipd)
+{
+	int cpuid, n;
+
+	for (cpuid = 0; cpuid < IPIPE_NR_CPUS; cpuid++) {
+		ipd->cpudata[cpuid].irq_pending_hi = 0;
+
+		for (n = 0; n < IPIPE_IRQ_IWORDS; n++)
+			ipd->cpudata[cpuid].irq_pending_lo[n] = 0;
+
+		for (n = 0; n < IPIPE_NR_IRQS; n++) {
+			ipd->cpudata[cpuid].irq_counters[n].total_hits = 0;
+			ipd->cpudata[cpuid].irq_counters[n].pending_hits = 0;
+		}
+	}
+
+	for (n = 0; n < IPIPE_NR_IRQS; n++) {
+		ipd->irqs[n].acknowledge = NULL;
+		ipd->irqs[n].handler = NULL;
+		ipd->irqs[n].control = IPIPE_PASS_MASK;	/* Pass but don't handle */
+	}
+
+	for (n = 0; n < IPIPE_NR_EVENTS; n++)
+		ipd->evhand[n] = NULL;
+
+	ipd->evself = 0;
+
+#ifdef CONFIG_SMP
+	ipd->irqs[IPIPE_CRITICAL_IPI].acknowledge = &__ipipe_ack_system_irq;
+	ipd->irqs[IPIPE_CRITICAL_IPI].handler = &__ipipe_do_critical_sync;
+	ipd->irqs[IPIPE_CRITICAL_IPI].cookie = NULL;
+	/* Immediately handle in the current domain but *never* pass */
+	ipd->irqs[IPIPE_CRITICAL_IPI].control =
+		IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK|IPIPE_SYSTEM_MASK;
+#endif	/* CONFIG_SMP */
+}
+
+void __ipipe_stall_root(void)
+{
+	ipipe_declare_cpuid;
+	unsigned long flags;
+
+	ipipe_get_cpu(flags); /* Care for migration. */
+
+	set_bit(IPIPE_STALL_FLAG, &ipipe_root_domain->cpudata[cpuid].status);
+
+#ifdef CONFIG_SMP
+	if (!__ipipe_pipeline_head_p(ipipe_root_domain))
+		ipipe_put_cpu(flags);
+#else /* CONFIG_SMP */
+	if (__ipipe_pipeline_head_p(ipipe_root_domain))
+		local_irq_disable_hw();
+#endif /* CONFIG_SMP */
+	ipipe_mark_domain_stall(ipipe_root_domain,cpuid);
+}
+
+void __ipipe_cleanup_domain(struct ipipe_domain *ipd)
+{
+	ipipe_unstall_pipeline_from(ipd);
+
+#ifdef CONFIG_SMP
+	{
+		int cpu;
+
+		for_each_online_cpu(cpu) {
+			while (ipd->cpudata[cpu].irq_pending_hi != 0)
+				cpu_relax();
+		}
+	}
+#endif	/* CONFIG_SMP */
+}
+
+void __ipipe_unstall_root(void)
+{
+	ipipe_declare_cpuid;
+
+	local_irq_disable_hw();
+
+	ipipe_load_cpuid();
+
+	__clear_bit(IPIPE_STALL_FLAG, &ipipe_root_domain->cpudata[cpuid].status);
+
+	ipipe_mark_domain_unstall(ipipe_root_domain, cpuid);
+
+	if (ipipe_root_domain->cpudata[cpuid].irq_pending_hi != 0)
+		__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+
+	local_irq_enable_hw();
+}
+
+unsigned long __ipipe_test_root(void)
+{
+	unsigned long flags, s;
+	ipipe_declare_cpuid;
+
+	ipipe_get_cpu(flags); /* Care for migration. */
+	s = test_bit(IPIPE_STALL_FLAG, &ipipe_root_domain->cpudata[cpuid].status);
+	ipipe_put_cpu(flags);
+
+	return s;
+}
+
+unsigned long __ipipe_test_and_stall_root(void)
+{
+	unsigned long flags, s;
+	ipipe_declare_cpuid;
+
+	ipipe_get_cpu(flags); /* Care for migration. */
+	s = test_and_set_bit(IPIPE_STALL_FLAG,
+			     &ipipe_root_domain->cpudata[cpuid].status);
+	ipipe_mark_domain_stall(ipipe_root_domain,cpuid);
+	ipipe_put_cpu(flags);
+
+	return s;
+}
+
+void fastcall __ipipe_restore_root(unsigned long flags)
+{
+	if (flags)
+		__ipipe_stall_root();
+	else
+		__ipipe_unstall_root();
+}
+
+/*
+ * ipipe_unstall_pipeline_from() -- Unstall the pipeline and
+ * synchronize pending interrupts for a given domain. See
+ * __ipipe_walk_pipeline() for more information.
+ */
+void fastcall ipipe_unstall_pipeline_from(struct ipipe_domain *ipd)
+{
+	struct ipipe_domain *this_domain;
+	struct list_head *pos;
+	unsigned long flags;
+	ipipe_declare_cpuid;
+
+	ipipe_lock_cpu(flags);
+
+	__clear_bit(IPIPE_STALL_FLAG, &ipd->cpudata[cpuid].status);
+
+	ipipe_mark_domain_unstall(ipd, cpuid);
+
+	this_domain = ipipe_percpu_domain[cpuid];
+
+	if (ipd == this_domain) {
+		if (ipd->cpudata[cpuid].irq_pending_hi != 0)
+			__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+
+		goto release_cpu_and_exit;
+	}
+
+	list_for_each(pos, &__ipipe_pipeline) {
+
+		struct ipipe_domain *next_domain =
+			list_entry(pos, struct ipipe_domain, p_link);
+
+		if (test_bit(IPIPE_STALL_FLAG,
+			     &next_domain->cpudata[cpuid].status))
+			break;	/* Stalled stage -- do not go further. */
+
+		if (next_domain->cpudata[cpuid].irq_pending_hi != 0) {
+
+			if (next_domain == this_domain)
+				__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+			else {
+				__ipipe_switch_to(this_domain, next_domain,
+						  cpuid);
+
+				ipipe_load_cpuid();	/* Processor might have changed. */
+
+				if (this_domain->cpudata[cpuid].
+				    irq_pending_hi != 0
+				    && !test_bit(IPIPE_STALL_FLAG,
+						 &this_domain->cpudata[cpuid].
+						 status))
+					__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+			}
+
+			break;
+		} else if (next_domain == this_domain)
+			break;
+	}
+
+release_cpu_and_exit:
+
+	if (__ipipe_pipeline_head_p(ipd))
+		local_irq_enable_hw();
+	else
+		ipipe_unlock_cpu(flags);
+}
+
+/*
+ * ipipe_suspend_domain() -- Suspend the current domain, switching to
+ * the next one which has pending work down the pipeline.
+ */
+void ipipe_suspend_domain(void)
+{
+	struct ipipe_domain *this_domain, *next_domain;
+	struct list_head *ln;
+	unsigned long flags;
+	ipipe_declare_cpuid;
+
+	ipipe_lock_cpu(flags);
+
+	this_domain = next_domain = ipipe_percpu_domain[cpuid];
+
+	__clear_bit(IPIPE_STALL_FLAG, &this_domain->cpudata[cpuid].status);
+
+	ipipe_mark_domain_unstall(this_domain, cpuid);
+
+	if (this_domain->cpudata[cpuid].irq_pending_hi != 0)
+		goto sync_stage;
+
+	for (;;) {
+		ln = next_domain->p_link.next;
+
+		if (ln == &__ipipe_pipeline)
+			break;
+
+		next_domain = list_entry(ln, struct ipipe_domain, p_link);
+
+		if (test_bit(IPIPE_STALL_FLAG,
+			     &next_domain->cpudata[cpuid].status))
+			break;
+
+		if (next_domain->cpudata[cpuid].irq_pending_hi == 0)
+			continue;
+
+		ipipe_percpu_domain[cpuid] = next_domain;
+
+sync_stage:
+
+		__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+
+		ipipe_load_cpuid();	/* Processor might have changed. */
+
+		if (ipipe_percpu_domain[cpuid] != next_domain)
+			/*
+			 * Something has changed the current domain under our
+			 * feet, recycling the register set; take note.
+			 */
+			this_domain = ipipe_percpu_domain[cpuid];
+	}
+
+	ipipe_percpu_domain[cpuid] = this_domain;
+
+	ipipe_unlock_cpu(flags);
+}
+
+/* ipipe_alloc_virq() -- Allocate a pipelined virtual/soft interrupt.
+ * Virtual interrupts are handled in exactly the same way than their
+ * hw-generated counterparts wrt pipelining.
+ */
+unsigned ipipe_alloc_virq(void)
+{
+	unsigned long flags, irq = 0;
+	int ipos;
+
+	spin_lock_irqsave_hw(&__ipipe_pipelock, flags);
+
+	if (__ipipe_virtual_irq_map != ~0) {
+		ipos = ffz(__ipipe_virtual_irq_map);
+		set_bit(ipos, &__ipipe_virtual_irq_map);
+		irq = ipos + IPIPE_VIRQ_BASE;
+	}
+
+	spin_unlock_irqrestore_hw(&__ipipe_pipelock, flags);
+
+	return irq;
+}
+
+/* ipipe_virtualize_irq() -- Attach a handler (and optionally a hw
+   acknowledge routine) to an interrupt for a given domain. */
+
+int ipipe_virtualize_irq(struct ipipe_domain *ipd,
+			 unsigned irq,
+			 ipipe_irq_handler_t handler,
+			 void *cookie,
+			 ipipe_irq_ackfn_t acknowledge,
+			 unsigned modemask)
+{
+	unsigned long flags;
+	int err;
+
+	if (irq >= IPIPE_NR_IRQS)
+		return -EINVAL;
+
+	if (ipd->irqs[irq].control & IPIPE_SYSTEM_MASK)
+		return -EPERM;
+
+	spin_lock_irqsave_hw(&__ipipe_pipelock, flags);
+
+	if (handler != NULL) {
+
+		if (handler == IPIPE_SAME_HANDLER) {
+			handler = ipd->irqs[irq].handler;
+			cookie = ipd->irqs[irq].cookie;
+
+			if (handler == NULL) {
+				err = -EINVAL;
+				goto unlock_and_exit;
+			}
+		} else if ((modemask & IPIPE_EXCLUSIVE_MASK) != 0 &&
+			   ipd->irqs[irq].handler != NULL) {
+			err = -EBUSY;
+			goto unlock_and_exit;
+		}
+
+		if ((modemask & (IPIPE_SHARED_MASK | IPIPE_PASS_MASK)) ==
+		    IPIPE_SHARED_MASK) {
+			err = -EINVAL;
+			goto unlock_and_exit;
+		}
+
+		if ((modemask & IPIPE_STICKY_MASK) != 0)
+			modemask |= IPIPE_HANDLE_MASK;
+	} else
+		modemask &=
+		    ~(IPIPE_HANDLE_MASK | IPIPE_STICKY_MASK |
+		      IPIPE_SHARED_MASK);
+
+	if (acknowledge == NULL) {
+		if ((modemask & IPIPE_SHARED_MASK) == 0)
+			/* Acknowledge handler unspecified -- this is ok in
+			   non-shared management mode, but we will force the use
+			   of the Linux-defined handler instead. */
+			acknowledge = ipipe_root_domain->irqs[irq].acknowledge;
+		else {
+			/* A valid acknowledge handler to be called in shared mode
+			   is required when declaring a shared IRQ. */
+			err = -EINVAL;
+			goto unlock_and_exit;
+		}
+	}
+
+	ipd->irqs[irq].handler = handler;
+	ipd->irqs[irq].cookie = cookie;
+	ipd->irqs[irq].acknowledge = acknowledge;
+	ipd->irqs[irq].control = modemask;
+
+	if (irq < NR_IRQS &&
+	    handler != NULL &&
+	    !ipipe_virtual_irq_p(irq) && (modemask & IPIPE_ENABLE_MASK) != 0) {
+		if (ipd != ipipe_current_domain) {
+			/* IRQ enable/disable state is domain-sensitive, so we may
+			   not change it for another domain. What is allowed
+			   however is forcing some domain to handle an interrupt
+			   source, by passing the proper 'ipd' descriptor which
+			   thus may be different from ipipe_current_domain. */
+			err = -EPERM;
+			goto unlock_and_exit;
+		}
+
+		__ipipe_enable_irq(irq);
+	}
+
+	err = 0;
+
+      unlock_and_exit:
+
+	spin_unlock_irqrestore_hw(&__ipipe_pipelock, flags);
+
+	return err;
+}
+
+/* ipipe_control_irq() -- Change modes of a pipelined interrupt for
+ * the current domain. */
+
+int ipipe_control_irq(unsigned irq, unsigned clrmask, unsigned setmask)
+{
+	struct ipipe_domain *ipd;
+	unsigned long flags;
+
+	if (irq >= IPIPE_NR_IRQS)
+		return -EINVAL;
+
+	ipd = ipipe_current_domain;
+
+	if (ipd->irqs[irq].control & IPIPE_SYSTEM_MASK)
+		return -EPERM;
+
+	if (((setmask | clrmask) & IPIPE_SHARED_MASK) != 0)
+		return -EINVAL;
+
+	if (ipd->irqs[irq].handler == NULL)
+		setmask &= ~(IPIPE_HANDLE_MASK | IPIPE_STICKY_MASK);
+
+	if ((setmask & IPIPE_STICKY_MASK) != 0)
+		setmask |= IPIPE_HANDLE_MASK;
+
+	if ((clrmask & (IPIPE_HANDLE_MASK | IPIPE_STICKY_MASK)) != 0)	/* If one goes, both go. */
+		clrmask |= (IPIPE_HANDLE_MASK | IPIPE_STICKY_MASK);
+
+	spin_lock_irqsave_hw(&__ipipe_pipelock, flags);
+
+	ipd->irqs[irq].control &= ~clrmask;
+	ipd->irqs[irq].control |= setmask;
+
+	if ((setmask & IPIPE_ENABLE_MASK) != 0)
+		__ipipe_enable_irq(irq);
+	else if ((clrmask & IPIPE_ENABLE_MASK) != 0)
+		__ipipe_disable_irq(irq);
+
+	spin_unlock_irqrestore_hw(&__ipipe_pipelock, flags);
+
+	return 0;
+}
+
+/* __ipipe_dispatch_event() -- Low-level event dispatcher. */
+
+int fastcall __ipipe_dispatch_event (unsigned event, void *data)
+{
+	struct ipipe_domain *start_domain, *this_domain, *next_domain;
+	struct list_head *pos, *npos;
+	unsigned long flags;
+	ipipe_declare_cpuid;
+	int propagate = 1;
+
+	ipipe_lock_cpu(flags);
+
+	start_domain = this_domain = ipipe_percpu_domain[cpuid];
+
+	list_for_each_safe(pos,npos,&__ipipe_pipeline) {
+
+		next_domain = list_entry(pos,struct ipipe_domain,p_link);
+
+		/*
+		 * Note: Domain migration may occur while running
+		 * event or interrupt handlers, in which case the
+		 * current register set is going to be recycled for a
+		 * different domain than the initiating one. We do
+		 * care for that, always tracking the current domain
+		 * descriptor upon return from those handlers.
+		 */
+		if (next_domain->evhand[event] != NULL)	{
+			ipipe_percpu_domain[cpuid] = next_domain;
+			ipipe_unlock_cpu(flags);
+			propagate = !next_domain->evhand[event](event,start_domain,data);
+			ipipe_lock_cpu(flags);
+			if (ipipe_percpu_domain[cpuid] != next_domain)
+				this_domain = ipipe_percpu_domain[cpuid];
+		}
+
+		if (next_domain != ipipe_root_domain &&	/* NEVER sync the root stage here. */
+		    next_domain->cpudata[cpuid].irq_pending_hi != 0 &&
+		    !test_bit(IPIPE_STALL_FLAG,&next_domain->cpudata[cpuid].status)) {
+			ipipe_percpu_domain[cpuid] = next_domain;
+			__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+			ipipe_load_cpuid();
+			if (ipipe_percpu_domain[cpuid] != next_domain)
+				this_domain = ipipe_percpu_domain[cpuid];
+		}
+
+		ipipe_percpu_domain[cpuid] = this_domain;
+
+		if (next_domain == this_domain || !propagate)
+			break;
+	}
+
+	ipipe_unlock_cpu(flags);
+
+	return !propagate;
+}
+
+#ifdef CONFIG_PROC_FS
+
+#include <linux/proc_fs.h>
+
+struct proc_dir_entry *ipipe_proc_root;
+
+static int __ipipe_version_info_proc(char *page,
+				     char **start,
+				     off_t off, int count, int *eof, void *data)
+{
+	int len = sprintf(page, "%s\n", IPIPE_VERSION_STRING);
+
+	len -= off;
+
+	if (len <= off + count)
+		*eof = 1;
+
+	*start = page + off;
+
+	if(len > count)
+		len = count;
+
+	if(len < 0)
+		len = 0;
+
+	return len;
+}
+
+static int __ipipe_common_info_proc(char *page,
+				    char **start,
+				    off_t off, int count, int *eof, void *data)
+{
+	struct ipipe_domain *ipd = (struct ipipe_domain *)data;
+	unsigned long ctlbits;
+	unsigned irq, _irq;
+	char *p = page;
+	int len;
+
+	spin_lock(&__ipipe_pipelock);
+
+	p += sprintf(p, "Priority=%d, Id=0x%.8x\n",
+		     ipd->priority, ipd->domid);
+	irq = 0;
+
+	while (irq < IPIPE_NR_IRQS) {
+		ctlbits =
+			(ipd->irqs[irq].
+			 control & (IPIPE_HANDLE_MASK | IPIPE_PASS_MASK |
+				    IPIPE_STICKY_MASK));
+		if (irq >= IPIPE_NR_XIRQS && !ipipe_virtual_irq_p(irq)) {
+			/*
+			 * There might be a hole between the last external
+			 * IRQ and the first virtual one; skip it.
+			 */
+			irq++;
+			continue;
+		}
+
+		if (ipipe_virtual_irq_p(irq)
+		    && !test_bit(irq - IPIPE_VIRQ_BASE,
+				 &__ipipe_virtual_irq_map)) {
+			/* Non-allocated virtual IRQ; skip it. */
+			irq++;
+			continue;
+		}
+
+		/*
+		 * Attempt to group consecutive IRQ numbers having the
+		 * same virtualization settings in a single line.
+		 */
+
+		_irq = irq;
+
+		while (++_irq < IPIPE_NR_IRQS) {
+			if (ipipe_virtual_irq_p(_irq) !=
+			    ipipe_virtual_irq_p(irq)
+			    || (ipipe_virtual_irq_p(_irq)
+				&& !test_bit(_irq - IPIPE_VIRQ_BASE,
+					     &__ipipe_virtual_irq_map))
+			    || ctlbits != (ipd->irqs[_irq].
+			     control & (IPIPE_HANDLE_MASK |
+					IPIPE_PASS_MASK |
+					IPIPE_STICKY_MASK)))
+				break;
+		}
+
+		if (_irq == irq + 1)
+			p += sprintf(p, "irq%u: ", irq);
+		else
+			p += sprintf(p, "irq%u-%u: ", irq, _irq - 1);
+
+		/*
+		 * Statuses are as follows:
+		 * o "accepted" means handled _and_ passed down the pipeline.
+		 * o "grabbed" means handled, but the interrupt might be
+		 * terminated _or_ passed down the pipeline depending on
+		 * what the domain handler asks for to the I-pipe.
+		 * o "passed" means unhandled by the domain but passed
+		 * down the pipeline.
+		 * o "discarded" means unhandled and _not_ passed down the
+		 * pipeline. The interrupt merely disappears from the
+		 * current domain down to the end of the pipeline.
+		 */
+		if (ctlbits & IPIPE_HANDLE_MASK) {
+			if (ctlbits & IPIPE_PASS_MASK)
+				p += sprintf(p, "accepted");
+			else
+				p += sprintf(p, "grabbed");
+		} else if (ctlbits & IPIPE_PASS_MASK)
+			p += sprintf(p, "passed");
+		else
+			p += sprintf(p, "discarded");
+
+		if (ctlbits & IPIPE_STICKY_MASK)
+			p += sprintf(p, ", sticky");
+
+		if (ipipe_virtual_irq_p(irq))
+			p += sprintf(p, ", virtual");
+
+		p += sprintf(p, "\n");
+
+		irq = _irq;
+	}
+
+	spin_unlock(&__ipipe_pipelock);
+
+	len = p - page;
+
+	if (len <= off + count)
+		*eof = 1;
+
+	*start = page + off;
+
+	len -= off;
+
+	if (len > count)
+		len = count;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+#ifdef CONFIG_IPIPE_STATS
+
+static int __ipipe_stat_info_proc(char *page,
+				  char **start,
+				  off_t off, int count, int *eof, void *data)
+{
+	struct ipipe_domain *ipd = (struct ipipe_domain *)data;
+	int len = 0, cpu, irq;
+	char *p = page;
+
+	p += sprintf(p,"> STALL TIME:\n");
+
+	for_each_online_cpu(cpu) {
+		unsigned long eip = ipd->stats[cpu].max_stall_eip;
+		char namebuf[KSYM_NAME_LEN+1];
+		unsigned long offset, size, t;
+		const char *name;
+		char *modname;
+
+		name = kallsyms_lookup(eip, &size, &offset, &modname, namebuf);
+		t = ipipe_tsc2ns(ipd->stats[cpu].max_stall_time);
+
+		if (name) {
+			if (modname)
+				p += sprintf(p,"CPU%d  %12lu  (%s+%#lx [%s])\n",
+					     cpu,t,name,offset,modname);
+			else
+				p += sprintf(p,"CPU%d  %12lu  (%s+%#lx)\n",
+					     cpu,t,name,offset);
+		}
+		else
+			p += sprintf(p,"CPU%d  %12lu  (%lx)\n",
+				     cpu,t,eip);
+	}
+
+	p += sprintf(p,"> PROPAGATION TIME:\nIRQ");
+
+	for_each_online_cpu(cpu) {
+		p += sprintf(p,"         CPU%d",cpu);
+	}
+
+	for (irq = 0; irq < IPIPE_NR_IRQS; irq++) {
+
+		unsigned long long t = 0;
+
+		for_each_online_cpu(cpu) {
+			t += ipd->stats[cpu].irq_stats[irq].max_delivery_time;
+		}
+
+		if (!t)
+			continue;
+
+		p += sprintf(p,"\n%3d:",irq);
+
+		for_each_online_cpu(cpu) {
+			p += sprintf(p,"%13lu",
+				     ipipe_tsc2ns(ipd->stats[cpu].irq_stats[irq].max_delivery_time));
+		}
+	}
+
+	p += sprintf(p,"\n");
+
+	len = p - page - off;
+	if (len <= off + count) *eof = 1;
+	*start = page + off;
+	if (len > count) len = count;
+	if (len < 0) len = 0;
+
+	return len;
+}
+
+#endif /* CONFIG_IPIPE_STATS */
+
+void __ipipe_add_domain_proc(struct ipipe_domain *ipd)
+{
+
+	create_proc_read_entry(ipd->name,0444,ipipe_proc_root,&__ipipe_common_info_proc,ipd);
+#ifdef CONFIG_IPIPE_STATS
+	{
+		char name[64];
+		snprintf(name,sizeof(name),"%s_stats",ipd->name);
+		create_proc_read_entry(name,0444,ipipe_proc_root,&__ipipe_stat_info_proc,ipd);
+	}
+#endif /* CONFIG_IPIPE_STATS */
+}
+
+void __ipipe_remove_domain_proc(struct ipipe_domain *ipd)
+{
+	remove_proc_entry(ipd->name,ipipe_proc_root);
+#ifdef CONFIG_IPIPE_STATS
+	{
+		char name[64];
+		snprintf(name,sizeof(name),"%s_stats",ipd->name);
+		remove_proc_entry(name,ipipe_proc_root);
+	}
+#endif /* CONFIG_IPIPE_STATS */
+}
+
+void ipipe_init_proc(void)
+{
+	ipipe_proc_root = create_proc_entry("ipipe",S_IFDIR, 0);
+	create_proc_read_entry("version",0444,ipipe_proc_root,&__ipipe_version_info_proc,NULL);
+	__ipipe_init_trace_proc();
+	__ipipe_add_domain_proc(ipipe_root_domain);
+}
+
+#endif	/* CONFIG_PROC_FS */
+
+EXPORT_SYMBOL(ipipe_virtualize_irq);
+EXPORT_SYMBOL(ipipe_control_irq);
+EXPORT_SYMBOL(ipipe_suspend_domain);
+EXPORT_SYMBOL(ipipe_alloc_virq);
+EXPORT_SYMBOL(ipipe_unstall_pipeline_from);
+EXPORT_SYMBOL(ipipe_percpu_domain);
+EXPORT_SYMBOL(ipipe_root_domain);
+EXPORT_SYMBOL(__ipipe_unstall_root);
+EXPORT_SYMBOL(__ipipe_stall_root);
+EXPORT_SYMBOL(__ipipe_restore_root);
+EXPORT_SYMBOL(__ipipe_test_and_stall_root);
+EXPORT_SYMBOL(__ipipe_test_root);
+EXPORT_SYMBOL(__ipipe_dispatch_event);
+EXPORT_SYMBOL(__ipipe_pipeline);
+EXPORT_SYMBOL(__ipipe_pipelock);
+EXPORT_SYMBOL(__ipipe_virtual_irq_map);
diff -uNrp 2.6.12/kernel/ipipe/generic.c 2.6.12-ipipe/kernel/ipipe/generic.c
--- 2.6.12/kernel/ipipe/generic.c	1970-01-01 01:00:00.000000000 +0100
+++ 2.6.12-ipipe/kernel/ipipe/generic.c	2006-01-06 09:27:24.000000000 +0100
@@ -0,0 +1,397 @@
+/* -*- linux-c -*-
+ * linux/kernel/ipipe/generic.c
+ *
+ * Copyright (C) 2002-2005 Philippe Gerum.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ * USA; either version 2 of the License, or (at your option) any later
+ * version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Architecture-independent I-PIPE services.
+ */
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#endif	/* CONFIG_PROC_FS */
+
+MODULE_DESCRIPTION("I-pipe");
+MODULE_LICENSE("GPL");
+
+static int __ipipe_ptd_key_count;
+
+static unsigned long __ipipe_ptd_key_map;
+
+/* ipipe_register_domain() -- Link a new domain to the pipeline. */
+
+int ipipe_register_domain(struct ipipe_domain *ipd,
+			  struct ipipe_domain_attr *attr)
+{
+	struct list_head *pos;
+	unsigned long flags;
+
+	if (ipipe_current_domain != ipipe_root_domain) {
+		printk(KERN_WARNING
+		       "I-pipe: Only the root domain may register a new domain.\n");
+		return -EPERM;
+	}
+
+	flags = ipipe_critical_enter(NULL);
+
+	list_for_each(pos, &__ipipe_pipeline) {
+		struct ipipe_domain *_ipd =
+			list_entry(pos, struct ipipe_domain, p_link);
+		if (_ipd->domid == attr->domid)
+			break;
+	}
+
+	ipipe_critical_exit(flags);
+
+	if (pos != &__ipipe_pipeline)
+		/* A domain with the given id already exists -- fail. */
+		return -EBUSY;
+
+	ipd->name = attr->name;
+	ipd->priority = attr->priority;
+	ipd->domid = attr->domid;
+	ipd->pdd = attr->pdd;
+	ipd->flags = 0;
+
+#ifdef CONFIG_IPIPE_STATS
+	{
+		int cpu, irq;
+		for_each_online_cpu(cpu) {
+			ipd->stats[cpu].last_stall_date = 0LL;
+			for (irq = 0; irq < IPIPE_NR_IRQS; irq++)
+				ipd->stats[cpu].irq_stats[irq].last_receipt_date = 0LL;
+		}
+	}
+#endif /* CONFIG_IPIPE_STATS */
+
+	__ipipe_init_stage(ipd);
+
+	INIT_LIST_HEAD(&ipd->p_link);
+
+#ifdef CONFIG_PROC_FS
+	__ipipe_add_domain_proc(ipd);
+#endif /* CONFIG_PROC_FS */
+
+	flags = ipipe_critical_enter(NULL);
+
+	list_for_each(pos, &__ipipe_pipeline) {
+		struct ipipe_domain *_ipd =
+			list_entry(pos, struct ipipe_domain, p_link);
+		if (ipd->priority > _ipd->priority)
+			break;
+	}
+
+	list_add_tail(&ipd->p_link, pos);
+
+	ipipe_critical_exit(flags);
+
+	printk(KERN_WARNING "I-pipe: Domain %s registered.\n", ipd->name);
+
+	/*
+	 * Finally, allow the new domain to perform its initialization
+	 * chores.
+	 */
+
+	if (attr->entry != NULL) {
+		ipipe_declare_cpuid;
+
+		ipipe_lock_cpu(flags);
+
+		ipipe_percpu_domain[cpuid] = ipd;
+		attr->entry();
+		ipipe_percpu_domain[cpuid] = ipipe_root_domain;
+
+		ipipe_load_cpuid();	/* Processor might have changed. */
+
+		if (ipipe_root_domain->cpudata[cpuid].irq_pending_hi != 0 &&
+		    !test_bit(IPIPE_STALL_FLAG,
+			      &ipipe_root_domain->cpudata[cpuid].status))
+			__ipipe_sync_stage(IPIPE_IRQMASK_ANY);
+
+		ipipe_unlock_cpu(flags);
+	}
+
+	return 0;
+}
+
+/* ipipe_unregister_domain() -- Remove a domain from the pipeline. */
+
+int ipipe_unregister_domain(struct ipipe_domain *ipd)
+{
+	unsigned long flags;
+
+	if (ipipe_current_domain != ipipe_root_domain) {
+		printk(KERN_WARNING
+		       "I-pipe: Only the root domain may unregister a domain.\n");
+		return -EPERM;
+	}
+
+	if (ipd == ipipe_root_domain) {
+		printk(KERN_WARNING
+		       "I-pipe: Cannot unregister the root domain.\n");
+		return -EPERM;
+	}
+#ifdef CONFIG_SMP
+	{
+		int nr_cpus = num_online_cpus(), _cpuid;
+		unsigned irq;
+
+		/*
+		 * In the SMP case, wait for the logged events to drain on
+		 * other processors before eventually removing the domain
+		 * from the pipeline.
+		 */
+
+		ipipe_unstall_pipeline_from(ipd);
+
+		flags = ipipe_critical_enter(NULL);
+
+		for (irq = 0; irq < IPIPE_NR_IRQS; irq++) {
+			clear_bit(IPIPE_HANDLE_FLAG, &ipd->irqs[irq].control);
+			clear_bit(IPIPE_STICKY_FLAG, &ipd->irqs[irq].control);
+			set_bit(IPIPE_PASS_FLAG, &ipd->irqs[irq].control);
+		}
+
+		ipipe_critical_exit(flags);
+
+		for (_cpuid = 0; _cpuid < nr_cpus; _cpuid++)
+			for (irq = 0; irq < IPIPE_NR_IRQS; irq++)
+				while (ipd->cpudata[_cpuid].irq_counters[irq].pending_hits > 0)
+					cpu_relax();
+	}
+#endif	/* CONFIG_SMP */
+
+#ifdef CONFIG_PROC_FS
+	__ipipe_remove_domain_proc(ipd);
+#endif /* CONFIG_PROC_FS */
+
+	/*
+	 * Simply remove the domain from the pipeline and we are almost done.
+	 */
+
+	flags = ipipe_critical_enter(NULL);
+	list_del_init(&ipd->p_link);
+	ipipe_critical_exit(flags);
+
+	__ipipe_cleanup_domain(ipd);
+
+	printk(KERN_WARNING "I-pipe: Domain %s unregistered.\n", ipd->name);
+
+	return 0;
+}
+
+/*
+ * ipipe_propagate_irq() -- Force a given IRQ propagation on behalf of
+ * a running interrupt handler to the next domain down the pipeline.
+ * ipipe_schedule_irq() -- Does almost the same as above, but attempts
+ * to pend the interrupt for the current domain first.
+ */
+int fastcall __ipipe_schedule_irq(unsigned irq, struct list_head *head)
+{
+	struct list_head *ln;
+	unsigned long flags;
+	ipipe_declare_cpuid;
+
+	if (irq >= IPIPE_NR_IRQS ||
+	    (ipipe_virtual_irq_p(irq)
+	     && !test_bit(irq - IPIPE_VIRQ_BASE, &__ipipe_virtual_irq_map)))
+		return -EINVAL;
+
+	ipipe_lock_cpu(flags);
+
+	ln = head;
+
+	while (ln != &__ipipe_pipeline) {
+		struct ipipe_domain *ipd =
+			list_entry(ln, struct ipipe_domain, p_link);
+
+		if (test_bit(IPIPE_HANDLE_FLAG, &ipd->irqs[irq].control)) {
+			ipd->cpudata[cpuid].irq_counters[irq].total_hits++;
+			ipd->cpudata[cpuid].irq_counters[irq].pending_hits++;
+			__ipipe_set_irq_bit(ipd, cpuid, irq);
+			ipipe_mark_irq_receipt(ipd, irq, cpuid);
+			ipipe_unlock_cpu(flags);
+			return 1;
+		}
+
+		ln = ipd->p_link.next;
+	}
+
+	ipipe_unlock_cpu(flags);
+
+	return 0;
+}
+
+/* ipipe_free_virq() -- Release a virtual/soft interrupt. */
+
+int ipipe_free_virq(unsigned virq)
+{
+	if (!ipipe_virtual_irq_p(virq))
+		return -EINVAL;
+
+	clear_bit(virq - IPIPE_VIRQ_BASE, &__ipipe_virtual_irq_map);
+
+	return 0;
+}
+
+void ipipe_init_attr(struct ipipe_domain_attr *attr)
+{
+	attr->name = "anon";
+	attr->domid = 1;
+	attr->entry = NULL;
+	attr->priority = IPIPE_ROOT_PRIO;
+	attr->pdd = NULL;
+}
+
+/*
+ * ipipe_catch_event() -- Interpose or remove an event handler for a
+ * given domain.
+ */
+int ipipe_catch_event(struct ipipe_domain *ipd,
+		      unsigned event,
+		      int (*handler)(unsigned event, struct ipipe_domain *ipd, void *data))
+{
+	int self = 0;
+
+	if (event & IPIPE_EVENT_SELF) {
+		event &= ~IPIPE_EVENT_SELF;
+		self = 1;
+	}
+
+	if (event >= IPIPE_NR_EVENTS)
+		return -EINVAL;
+
+	if (!xchg(&ipd->evhand[event],handler))	{
+		if (handler) {
+			if (self)
+				ipd->evself |= (1LL << event);
+			else
+				__ipipe_event_monitors[event]++;
+		}
+	}
+	else if (!handler) {
+		if (ipd->evself & (1LL << event))
+			ipd->evself &= ~(1LL << event);
+		else
+			__ipipe_event_monitors[event]--;
+	} else if ((ipd->evself & (1LL << event)) && !self) {
+			__ipipe_event_monitors[event]++;
+			ipd->evself &= ~(1LL << event);
+	} else if (!(ipd->evself & (1LL << event)) && self) {
+			__ipipe_event_monitors[event]--;
+			ipd->evself |= (1LL << event);
+	}
+
+	return 0;
+}
+
+cpumask_t ipipe_set_irq_affinity (unsigned irq, cpumask_t cpumask)
+{
+#ifdef CONFIG_SMP
+	if (irq >= IPIPE_NR_XIRQS)
+		/* Allow changing affinity of external IRQs only. */
+		return CPU_MASK_NONE;
+
+	if (num_online_cpus() > 1)
+		return __ipipe_set_irq_affinity(irq,cpumask);
+#endif /* CONFIG_SMP */
+
+	return CPU_MASK_NONE;
+}
+
+int fastcall ipipe_send_ipi (unsigned ipi, cpumask_t cpumask)
+
+{
+#ifdef CONFIG_SMP
+	return __ipipe_send_ipi(ipi,cpumask);
+#else /* !CONFIG_SMP */
+	return -EINVAL;
+#endif /* CONFIG_SMP */
+}
+
+int ipipe_alloc_ptdkey (void)
+{
+	unsigned long flags;
+	int key = -1;
+
+	spin_lock_irqsave_hw(&__ipipe_pipelock,flags);
+
+	if (__ipipe_ptd_key_count < IPIPE_ROOT_NPTDKEYS) {
+		key = ffz(__ipipe_ptd_key_map);
+		set_bit(key,&__ipipe_ptd_key_map);
+		__ipipe_ptd_key_count++;
+	}
+
+	spin_unlock_irqrestore_hw(&__ipipe_pipelock,flags);
+
+	return key;
+}
+
+int ipipe_free_ptdkey (int key)
+{
+	unsigned long flags;
+
+	if (key < 0 || key >= IPIPE_ROOT_NPTDKEYS)
+		return -EINVAL;
+
+	spin_lock_irqsave_hw(&__ipipe_pipelock,flags);
+
+	if (test_and_clear_bit(key,&__ipipe_ptd_key_map))
+		__ipipe_ptd_key_count--;
+
+	spin_unlock_irqrestore_hw(&__ipipe_pipelock,flags);
+
+	return 0;
+}
+
+int fastcall ipipe_set_ptd (int key, void *value)
+
+{
+	if (key < 0 || key >= IPIPE_ROOT_NPTDKEYS)
+		return -EINVAL;
+
+	current->ptd[key] = value;
+
+	return 0;
+}
+
+void fastcall *ipipe_get_ptd (int key)
+
+{
+	if (key < 0 || key >= IPIPE_ROOT_NPTDKEYS)
+		return NULL;
+
+	return current->ptd[key];
+}
+
+EXPORT_SYMBOL(ipipe_register_domain);
+EXPORT_SYMBOL(ipipe_unregister_domain);
+EXPORT_SYMBOL(ipipe_free_virq);
+EXPORT_SYMBOL(ipipe_init_attr);
+EXPORT_SYMBOL(ipipe_catch_event);
+EXPORT_SYMBOL(ipipe_alloc_ptdkey);
+EXPORT_SYMBOL(ipipe_free_ptdkey);
+EXPORT_SYMBOL(ipipe_set_ptd);
+EXPORT_SYMBOL(ipipe_get_ptd);
+EXPORT_SYMBOL(ipipe_set_irq_affinity);
+EXPORT_SYMBOL(ipipe_send_ipi);
+EXPORT_SYMBOL(__ipipe_schedule_irq);
diff -uNrp 2.6.12/kernel/printk.c 2.6.12-ipipe/kernel/printk.c
--- 2.6.12/kernel/printk.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/printk.c	2006-01-04 10:47:50.000000000 +0100
@@ -502,6 +502,79 @@ __setup("time", printk_time_setup);
  * is inspected when the actual printing occurs.
  */
 
+#ifdef CONFIG_IPIPE
+
+static ipipe_spinlock_t __ipipe_printk_lock = IPIPE_SPIN_LOCK_UNLOCKED;
+
+static int __ipipe_printk_fill;
+
+static char __ipipe_printk_buf[__LOG_BUF_LEN];
+
+void __ipipe_flush_printk (unsigned virq, void *cookie)
+{
+	char *p = __ipipe_printk_buf;
+	int len, lmax, out = 0;
+	unsigned long flags;
+
+	goto start;
+
+	do {
+		spin_unlock_irqrestore_hw(&__ipipe_printk_lock,flags);
+ start:
+		lmax = __ipipe_printk_fill;
+		while (out < lmax) {
+			len = strlen(p) + 1;
+			printk("%s",p);
+			p += len;
+			out += len;
+		}
+		spin_lock_irqsave_hw(&__ipipe_printk_lock,flags);
+	}
+	while (__ipipe_printk_fill != lmax);
+
+	__ipipe_printk_fill = 0;
+
+	spin_unlock_irqrestore_hw(&__ipipe_printk_lock,flags);
+}
+
+asmlinkage int printk(const char *fmt, ...)
+{
+	int r, fbytes, oldcount;
+    	unsigned long flags;
+	va_list args;
+
+	va_start(args, fmt);
+
+	if (ipipe_current_domain == ipipe_root_domain ||
+	    test_bit(IPIPE_SPRINTK_FLAG,&ipipe_current_domain->flags) ||
+	    oops_in_progress) {
+		r = vprintk(fmt, args);
+		goto out;
+	}
+
+	spin_lock_irqsave_hw(&__ipipe_printk_lock,flags);
+
+	oldcount = __ipipe_printk_fill;
+	fbytes = __LOG_BUF_LEN - oldcount;
+
+	if (fbytes > 1)	{
+		r = vscnprintf(__ipipe_printk_buf + __ipipe_printk_fill,
+			       fbytes, fmt, args) + 1; /* account for the null byte */
+		__ipipe_printk_fill += r;
+	} else
+		r = 0;
+
+	spin_unlock_irqrestore_hw(&__ipipe_printk_lock,flags);
+
+	if (oldcount == 0)
+		ipipe_trigger_irq(__ipipe_printk_virq);
+out: 
+	va_end(args);
+
+	return r;
+}
+#else /* !CONFIG_IPIPE */
+
 asmlinkage int printk(const char *fmt, ...)
 {
 	va_list args;
@@ -514,6 +587,8 @@ asmlinkage int printk(const char *fmt, .
 	return r;
 }
 
+#endif /* CONFIG_IPIPE */
+
 asmlinkage int vprintk(const char *fmt, va_list args)
 {
 	unsigned long flags;
diff -uNrp 2.6.12/kernel/sched.c 2.6.12-ipipe/kernel/sched.c
--- 2.6.12/kernel/sched.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/sched.c	2005-12-28 12:20:45.000000000 +0100
@@ -1343,10 +1343,14 @@ static inline void finish_task_switch(ta
 asmlinkage void schedule_tail(task_t *prev)
 	__releases(rq->lock)
 {
+	ipipe_stall_root_raw();
+
 	finish_task_switch(prev);
 
 	if (current->set_child_tid)
 		put_user(current->pid, current->set_child_tid);
+
+	ipipe_unstall_root_raw();
 }
 
 /*
@@ -2615,6 +2619,11 @@ asmlinkage void __sched schedule(void)
 	unsigned long run_time;
 	int cpu, idx;
 
+#ifdef CONFIG_IPIPE
+	if (unlikely(ipipe_current_domain != ipipe_root_domain)) {
+		goto need_resched;
+	}
+#endif /* CONFIG_IPIPE */
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * schedule() atomically, we ignore that path for now.
@@ -2631,6 +2640,12 @@ asmlinkage void __sched schedule(void)
 	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
 
 need_resched:
+#ifdef CONFIG_IPIPE
+	if (unlikely(ipipe_current_domain != ipipe_root_domain)) {
+		preempt_enable();
+		return;
+	}
+#endif /* CONFIG_IPIPE */
 	preempt_disable();
 	prev = current;
 	release_kernel_lock(prev);
@@ -2769,6 +2784,8 @@ switch_tasks:
 		prepare_arch_switch(rq, next);
 		prev = context_switch(rq, prev, next);
 		barrier();
+ 		if (task_hijacked(prev))
+ 		    return;
 
 		finish_task_switch(prev);
 	} else
@@ -2797,6 +2814,11 @@ asmlinkage void __sched preempt_schedule
 	struct task_struct *task = current;
 	int saved_lock_depth;
 #endif
+#ifdef CONFIG_IPIPE
+	/* Do not reschedule over non-Linux domains. */
+	if (ipipe_current_domain != ipipe_root_domain)
+		return;
+#endif /* CONFIG_IPIPE */
 	/*
 	 * If there is a non-zero preempt_count or interrupts are disabled,
 	 * we do not want to preempt the current task.  Just return..
@@ -3416,6 +3438,7 @@ recheck:
 		deactivate_task(p, rq);
 	oldprio = p->prio;
 	__setscheduler(p, policy, param->sched_priority);
+	ipipe_setsched_notify(p);
 	if (array) {
 		__activate_task(p, rq);
 		/*
@@ -5024,3 +5047,53 @@ void normalize_rt_tasks(void)
 }
 
 #endif /* CONFIG_MAGIC_SYSRQ */
+
+#ifdef CONFIG_IPIPE
+
+int ipipe_setscheduler_root (struct task_struct *p, int policy, int prio)
+{
+	prio_array_t *array;
+	unsigned long flags;
+	runqueue_t *rq;
+	int oldprio;
+
+	if (prio < 1 || prio > MAX_RT_PRIO-1)
+		return -EINVAL;
+
+	rq = task_rq_lock(p, &flags);
+	array = p->array;
+	if (array)
+		deactivate_task(p, rq);
+	oldprio = p->prio;
+	__setscheduler(p, policy, prio);
+	if (array) {
+		__activate_task(p, rq);
+		if (task_running(rq, p)) {
+			if (p->prio > oldprio)
+				resched_task(rq->curr);
+		} else if (TASK_PREEMPTS_CURR(p, rq))
+			resched_task(rq->curr);
+	}
+	task_rq_unlock(rq, &flags);
+
+	return 0;
+}
+
+EXPORT_SYMBOL(ipipe_setscheduler_root);
+
+int ipipe_reenter_root (struct task_struct *prev, int policy, int prio)
+{
+	finish_task_switch(prev);
+	if (reacquire_kernel_lock(current) < 0)
+		;
+	preempt_enable_no_resched();
+
+	if (current->policy != policy || current->rt_priority != prio)
+		return ipipe_setscheduler_root(current,policy,prio);
+
+	return 0;
+}
+
+EXPORT_SYMBOL(ipipe_reenter_root);
+
+#endif /* CONFIG_IPIPE */
diff -uNrp 2.6.12/kernel/signal.c 2.6.12-ipipe/kernel/signal.c
--- 2.6.12/kernel/signal.c	2005-08-12 06:12:04.000000000 +0200
+++ 2.6.12-ipipe/kernel/signal.c	2005-12-28 11:48:45.000000000 +0100
@@ -610,6 +610,7 @@ void signal_wake_up(struct task_struct *
 	unsigned int mask;
 
 	set_tsk_thread_flag(t, TIF_SIGPENDING);
+	ipipe_sigwake_notify(t); /* TIF_SIGPENDING must be set first. */
 
 	/*
 	 * For SIGKILL, we want to wake it up in the stopped/traced case.
Binary files 2.6.12/linux and 2.6.12-ipipe/linux differ
